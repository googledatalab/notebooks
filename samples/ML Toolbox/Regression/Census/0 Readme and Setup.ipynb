{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income Prediction using a Census Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This directory contains a series of notebooks that demonstrate building a model to predict income using US census data.\n",
    "\n",
    "The model will be built using [TensorFlow](https://tensorflow.org) and the Google Cloud Datalab Machine Learning Toolbox, which contains out-of-the-box models that can be easily applied to your data. In this sample, a regression model (a model that can be used to predict a continuous value) will be used. The specific type of regression model chosen for this sample is one that is implemented as a deep neural network.\n",
    "\n",
    "The notebooks will use [Google Cloud Machine Learning Engine](https://cloud.google.com/ml-engine) to submit training jobs to train the model, and to deploy the resulting model for predictions. The notebooks will also use Google BigQuery and Google Cloud Dataflow at appropriate points in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Data\n",
    "\n",
    "The sample uses the census/population datasets from the [American Community Survey](https://www.census.gov/programs-surveys/acs/technical-documentation/pums/documentation.2014.html).\n",
    "\n",
    "* [US-wide population from 2014](http://www2.census.gov/programs-surveys/acs/data/pums/2014/1-Year/csv_psd.zip).\n",
    "* [Detailed description](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict14.txt) of the data, including the columns, and values.\n",
    "\n",
    "A copy of this data, extracted from the original zip file has been copied to Google Cloud Storage as a publicly accessible object at `gs://cloud-datalab-samples/census/ss14psd.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow and Notebook Guide\n",
    "\n",
    "A typical machine learning workflow spans multiple stages, and is iterative in nature, starting with data preparation and exploration, continuing on to training and model evaluation, and then deployment before the model is used in applications or other data pipelines to produce predictions.\n",
    "\n",
    "While practicing this workflow, the recommendation is to work with sample data and develop the model in the local Datalab environment before launching large scale jobs on Cloud. On Cloud, the services are optimized for large scale, which is essential for completing the task and harnessing the value of the entire dataset, but this can introduce hurdles in quick develop-test-validate iteration, as well as introduce latencies that get in the way of iteration on sample data.\n",
    "\n",
    "The notebooks in this directory reflect this workflow.\n",
    "\n",
    "1. **[Local End to End](./1 Local End to End.ipynb)** - demonstrates the end-to-end development workflow, running locally in the Datalab environment.\n",
    "2. **[Service Preprocess](./2 Service Preprocess.ipynb)** - demonstrates data preparation and data analysis using data in Google Cloud Storage and BigQuery.\n",
    "3. **[Service Train](./3 Service Train.ipynb)** - demonstrates training a model using Machine Learning Engine.\n",
    "4. **[Service Evaluate](./4 Service Evaluate.ipynb)** - demonstrates evaluating the resulting model using Dataflow and BigQuery.\n",
    "5. **[Service Predict](./5 Service Predict.ipynb)** - demonstrates deploying the model to Machine Learning Engine and using both online and batch prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
