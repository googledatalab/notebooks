{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Preprocessing with BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the first of a set of steps to run machine learning on the cloud. This step is about data preparation and preprocessing, and will mirror the equivalent portions of the local notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace Setup\n",
    "\n",
    "The first step is to setup the workspace that we will use within this notebook - the python libraries, and the Google Cloud Storage bucket that will be used to contain the inputs and outputs produced over the course of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.datalab as datalab\n",
    "import google.datalab.ml as ml\n",
    "import mltoolbox.regression.dnn as regression\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The storage bucket we create will be created by default using the project id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storage_bucket = 'gs://' + datalab.Context.default().project_id + '-datalab-workspace/'\n",
    "storage_region = 'us-central1'\n",
    "\n",
    "workspace_path = os.path.join(storage_bucket, 'census')\n",
    "\n",
    "# We will rely on outputs from data preparation steps in the previous notebook.\n",
    "local_workspace_path = '/content/datalab/workspace/census'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil mb -c regional -l {storage_region} {storage_bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you have previously run this notebook, and want to start from scratch, then you run the next cell to delete and create previous outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil -m rm -rf {workspace_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will copy the data into this workspace from the local workspace created in the previous notebook.\n",
    "\n",
    "Generally, in your own work, you will have existing data to work with, that you may or may not need to copy around, depending on its current location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-users-datalab-workspace/census/:\n",
      "\n",
      "gs://cloud-ml-users-datalab-workspace/census/data/:\n",
      "gs://cloud-ml-users-datalab-workspace/census/data/eval.csv\n",
      "gs://cloud-ml-users-datalab-workspace/census/data/schema.json\n",
      "gs://cloud-ml-users-datalab-workspace/census/data/train.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q cp {local_workspace_path}/data/train.csv {workspace_path}/data/train.csv\n",
    "!gsutil -q cp {local_workspace_path}/data/eval.csv {workspace_path}/data/eval.csv\n",
    "!gsutil -q cp {local_workspace_path}/data/schema.json {workspace_path}/data/schema.json\n",
    "!gsutil ls -r {workspace_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_path = os.path.join(workspace_path, 'data/train.csv')\n",
    "eval_data_path = os.path.join(workspace_path, 'data/eval.csv')\n",
    "schema_path = os.path.join(workspace_path, 'data/schema.json')\n",
    "\n",
    "train_data = ml.CsvDataSet(file_pattern=train_data_path, schema_file=schema_path)\n",
    "eval_data = ml.CsvDataSet(file_pattern=eval_data_path, schema_file=schema_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "When building a model, a number of pieces of information about the training data are required - for example, the list of entries or vocabulary of a categorical/discrete column, or aggregate statistics like min and max for numerical columns. These require a full pass over the training data, and is usually done once, and needs to be repeated once if you change the schema in a future iteration.\n",
    "\n",
    "On the Cloud, this analysis is done with BigQuery, by referencing the csv data in storage as external data sources. The output of this analysis will be stored into storage.\n",
    "\n",
    "In the `analyze()` call below, notice the use of `cloud=True` to move data analysis from happening locally to happen in cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track BigQuery status at\n",
      "https://bigquery.cloud.google.com/queries/cloud-ml-users\n",
      "Running numerical analysis...done.\n",
      "Running categorical analysis...done.\n",
      "Analyze: completed\n"
     ]
    }
   ],
   "source": [
    "analysis_path = os.path.join(workspace_path, 'analysis')\n",
    "\n",
    "regression.analyze(dataset=train_data, output_dir=analysis_path, cloud=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the local notebook, the output of analysis is a numerical_analysis file that contains analysis from the numerical columns, and a vocab file from each categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-users-datalab-workspace/census/analysis/schema.json\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/stats.json\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_AGEP.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_COW.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_ESP.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_ESR.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_FOD1P.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_HINS4.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_INDP.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_JWMNP.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_JWTR.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_MAR.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_POWPUMA.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_PUMA.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_RAC1P.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_SCHL.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_SCIENGRLP.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_SERIALNO.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_SEX.csv\r\n",
      "gs://cloud-ml-users-datalab-workspace/census/analysis/vocab_WKW.csv\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {analysis_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect one of the files; in particular the numerical analysis, since it will also tell us some interesting statistics about the income column, the value we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"WAGP\": {\r\n",
      "    \"max\": 145.0,\r\n",
      "    \"mean\": 37.818628158844774,\r\n",
      "    \"min\": 10.0\r\n",
      "  }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!gsutil cat {analysis_path}/stats.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "This notebook completed the first steps of our machine learning workflow - data preparation and analysis. This data and the analysis outputs will be used to train a model, which is covered in the [next notebook](./3 Service Train.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
