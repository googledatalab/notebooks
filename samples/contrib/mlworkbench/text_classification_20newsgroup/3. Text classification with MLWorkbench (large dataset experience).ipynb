{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLWorkbench Magics\n",
    "\n",
    "This notebook does the same thing as the previous notebook, but uses cloud services for each step. The goal is to show how the MLWorkbench magic are used differently when using ML Engine and other GCP products. This notebook does not cover MLWorkbench is detail--see the previous notebook--but points how what changes when running on the cloud. \n",
    "\n",
    "If you changed the WORKSPACE_PATH variable in the previous notebook, you must also change it here. If you made no modifications, there is no need to update the next cell. The previous notebook must be executed before this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = '/content/datalab/workspace/text_classification_20newsgroup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What changes from local to cloud usage of the MLWorkbench magics?\n",
    "\n",
    "Generally, a few things need to change:\n",
    "\n",
    "* all data sources or file paths must be on GCS\n",
    "* the --cloud flag must be set\n",
    "* optional cloud_config values can be set\n",
    "\n",
    "Other than this, nothing else changes from local to cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Move the data to GCS\n",
    "\n",
    "The csv files, and all input files to the MLWorkbench magics must exist on GCS first. Therefore the first step is to make a new GCS bucket and copy the local csv files to GCS. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a bucket name. This bucket name should not exist.\n",
    "gcs_bucket = 'gs://' + datalab_project_id() + '-mlworkbench-20news-lab' # Feel free to change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells will make the bucket and copy the clean test and train csv files over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://cloud-ml-dev-mlworkbench-20news-lab/...\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb $gcs_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/datalab/workspace/text_classification_20newsgroup/news_clean_train.csv [Content-Type=text/csv]...\n",
      "Copying file:///content/datalab/workspace/text_classification_20newsgroup/news_clean_test.csv [Content-Type=text/csv]...\n",
      "- [2/2 files][ 10.7 MiB/ 10.7 MiB] 100% Done                                    \n",
      "Operation completed over 2 objects/10.7 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp $WORKSPACE_PATH/news_clean_train.csv $WORKSPACE_PATH/news_clean_test.csv $gcs_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the files are on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-dev-mlworkbench-20news-lab/news_clean_test.csv\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/news_clean_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $gcs_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.datalab.contrib.mlworkbench.commands  # This loads the '%%ml' magics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make some constant file paths\n",
    "\n",
    "# Input files\n",
    "train_csv_file = os.path.join(gcs_bucket, 'news_clean_train.csv')\n",
    "eval_csv_file = os.path.join(gcs_bucket, 'news_clean_test.csv')\n",
    "\n",
    "# For analyze step\n",
    "analyze_output = os.path.join(gcs_bucket, 'analyze_output')\n",
    "\n",
    "# For the transform step\n",
    "transform_output = os.path.join(gcs_bucket, 'transform_output')\n",
    "transformed_train_pattern = os.path.join(transform_output, 'features_train*')\n",
    "transformed_eval_pattern = os.path.join(transform_output, 'features_eval*')\n",
    "\n",
    "# For the training step\n",
    "training_output = os.path.join(gcs_bucket, 'training_output')\n",
    "\n",
    "# For the prediction steps\n",
    "batch_predict_output = os.path.join(gcs_bucket, 'batch_predict_output')\n",
    "evaluation_model = os.path.join(training_output, 'evaluation_model')\n",
    "regular_model = os.path.join(training_output, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deploy the model, we will create a ML Engine model and two versions. Change the names below if desired. The model and version names should not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlengine_model_name = 'datalab_mlworkbench_20news_model'\n",
    "mlengine_evaluation_version_name = 'evaluation_version'\n",
    "mlengine_regular_version_name = 'v1'\n",
    "\n",
    "full_evaluation_model_name = mlengine_model_name + '.' + mlengine_evaluation_version_name\n",
    "full_regular_model_name = mlengine_model_name + '.' + mlengine_regular_version_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Analyze the csv file\n",
    "The csv data must be on GCS. We copied the data in the above cells. To run analyze in the cloud, the csv file must be on GCS and the --cloud flag must be used. Cloud analyze will use BigQuery as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing column news_label...\n",
      "column news_label analyzed.\n",
      "Analyzing column text...\n",
      "Updated property [core/project].\n",
      "column text analyzed.\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "%%ml analyze --cloud\n",
    "output: $analyze_output\n",
    "training_data:\n",
    "  csv: $train_csv_file\n",
    "  schema:\n",
    "    - name: news_label\n",
    "      type: STRING\n",
    "    - name: text\n",
    "      type: STRING\n",
    "features:\n",
    "  news_label:\n",
    "    transform: target\n",
    "  text:\n",
    "    transform: bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls $analyze_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Transforming the input data\n",
    "\n",
    "The output, analyze, and csv parameters must all be GCS paths. Unlike analyze, running the transform step using cloud services supports cloud options which are passed to the DataFlow job. run '%%ml transform --help' for a list of cloud options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/apache_beam/io/gcp/gcsio.py:113: DeprecationWarning: object() takes no parameters\n",
      "  super(GcsIO, cls).__new__(cls, storage_client))\n",
      "/usr/local/lib/python2.7/dist-packages/apache_beam/coders/typecoders.py:135: UserWarning: Using fallback coder for typehint: Any.\n",
      "  warnings.warn('Using fallback coder for typehint: %r.' % typehint)\n",
      "running sdist\n",
      "running egg_info\n",
      "creating trainer.egg-info\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "creating trainer-1.0.0\n",
      "creating trainer-1.0.0/trainer\n",
      "creating trainer-1.0.0/trainer.egg-info\n",
      "copying files to trainer-1.0.0...\n",
      "copying setup.py -> trainer-1.0.0\n",
      "copying trainer/__init__.py -> trainer-1.0.0/trainer\n",
      "copying trainer/feature_transforms.py -> trainer-1.0.0/trainer\n",
      "copying trainer/task.py -> trainer-1.0.0/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-1.0.0/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-1.0.0/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-1.0.0/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-1.0.0/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-1.0.0/trainer.egg-info\n",
      "Writing trainer-1.0.0/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-1.0.0' (and everything under it)\n",
      "DEPRECATION: pip install --download has been deprecated and will be removed in the future. Pip now has a download command that should be used instead.\n",
      "Collecting google-cloud-dataflow==2.0.0\n",
      "  Downloading google-cloud-dataflow-2.0.0.tar.gz (576kB)\n",
      "  Saved /tmp/tmpLjdB7Q/google-cloud-dataflow-2.0.0.tar.gz\n",
      "Successfully downloaded google-cloud-dataflow\n",
      "View job at https://console.developers.google.com/dataflow/job/2017-07-25_16_36_11-8425675553938171601?project=cloud-ml-dev\n"
     ]
    }
   ],
   "source": [
    "%%ml transform --shuffle --cloud\n",
    "output: $transform_output\n",
    "analysis: $analyze_output\n",
    "prefix: features_train\n",
    "training_data:\n",
    "  csv: $train_csv_file\n",
    "cloud_config:\n",
    "  num_workers: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the above link to see the dataflow job. Note that control went back to the notebook--you can run other cells--but the dataflow job is still running. The job will take about 5-10 minutes. It is up to you to wait for the job to finish before continuing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to run transform on the eval set too. Because the dataset is small, dataflow's startup time is larger than the time it takes to run the transformation. So we run the next cell locally. If you wish, add --cloud to the next cell to run another dataflow job. As all paths are on GCS, the output will be on GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/apache_beam/io/gcp/gcsio.py:113: DeprecationWarning: object() takes no parameters\n",
      "  super(GcsIO, cls).__new__(cls, storage_client))\n",
      "2017-07-25 23:43:40.452248: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-07-25 23:43:40.452282: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-07-25 23:43:40.452309: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-07-25 23:43:40.452330: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-07-25 23:43:40.452345: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    }
   ],
   "source": [
    "%%ml transform \n",
    "output: $transform_output\n",
    "analysis: $analyze_output\n",
    "prefix: features_eval\n",
    "training_data:\n",
    "  csv: $eval_csv_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how more than 1 file may have been made for training and eval. Sharding an input file can improve TensorFlow training performance, especially when running on a distributed cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/errors_features_eval-00000-of-00001.txt\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/errors_features_train-00000-of-00001.txt\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00000-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00001-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00002-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00003-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00004-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00005-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00006-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00007-of-00008.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00000-of-00006.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00001-of-00006.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00002-of-00006.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00003-of-00006.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00004-of-00006.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00005-of-00006.tfrecord.gz\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/tmp/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $transform_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An errors file is always written, even if there are no errors. Let's check the error files are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 B  2017-07-25T23:43:57Z  gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/errors_features_eval-00000-of-00001.txt\r\n",
      "       0 B  2017-07-25T23:41:17Z  gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/errors_features_train-00000-of-00001.txt\r\n",
      "TOTAL: 2 objects, 0 bytes (0 B)\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -lh $transform_output/errors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training a TensorFlow model\n",
    "Again, see '%%ml train --help' for a list of cloud options. The cell below will run with default cloud options. Note that every file path must be a GCS path. You may want to change the cloud_config region value. Because the dataset is small, the cloud training will take more time than local training because of startup costs. It should take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\r\n"
     ]
    }
   ],
   "source": [
    "# Training should use an empty output folder. So if you run training multiple times,\n",
    "# use different folders or remove the output from the previous run.\n",
    "!gsutil -m rm -fr $training_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Job \"trainer_task_170725_234422\" submitted.<p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-ml-dev&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_170725_234422\" target=\"_blank\">here</a> to view cloud log. <br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml train --cloud\n",
    "output: $training_output\n",
    "analysis: $analyze_output\n",
    "training_data:\n",
    "  transformed: $transformed_train_pattern\n",
    "evaluation_data:\n",
    "  transformed: $transformed_eval_pattern\n",
    "model_args:\n",
    "  l2-regularization: 5\n",
    "  model: linear_classification\n",
    "  top-n: 4\n",
    "  learning-rate: 1\n",
    "  max-steps: 5000\n",
    "  train-batch-size: 500\n",
    "  eval-batch-size: 500\n",
    "  save-checkpoints-secs: 60\n",
    "cloud_config:\n",
    "  scale_tier: STANDARD_1\n",
    "  region: us-central1\n",
    "  runtime_version: '1.2'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is up to you to wait for the training job to finish before continuing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/schema_without_target.json\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/staging/\r\n",
      "gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls  $training_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Deploying the model\n",
    "\n",
    "See the previous notebook about the output models of training and the naming of ML Engine models. \n",
    "\n",
    "Below, we create a new ML Engine model, and two ML Engine model versions, one for each tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import Models, ModelVersions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'name': u'projects/cloud-ml-dev/models/datalab_mlworkbench_20news_model',\n",
       " u'regions': [u'us-central1']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Makes a ML Engine Model\n",
    "# If the model already exists, comment out this line\n",
    "Models().create(mlengine_model_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation \"projects/cloud-ml-dev/operations/create_datalab_mlworkbench_20news_model_v1-1501026791078\"\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Makes a ML Engine Version\n",
    "ModelVersions(mlengine_model_name).deploy(\n",
    "    version_name=mlengine_regular_version_name,\n",
    "    path=regular_model,\n",
    "    runtime_version='1.2')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation \"projects/cloud-ml-dev/operations/create_datalab_mlworkbench_20news_model_evaluation_version-1501027275238\"\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Makes a ML Engine Version\n",
    "ModelVersions(mlengine_model_name).deploy(\n",
    "    version_name=mlengine_evaluation_version_name,\n",
    "    path=evaluation_model,\n",
    "    runtime_version='1.2')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluation using batch prediction\n",
    "\n",
    "In the example below, we will run evaluation on the deployed evaluation model. Note the output and input file paths are on GCS. Also, model is not a path, it is the name of the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Job \"mlworkbench_batch_prediction_job_name_3\" submitted.<p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-ml-dev&resource=ml.googleapis.com%2Fjob_id%2Fmlworkbench_batch_prediction_job_name_3\" target=\"_blank\">here</a> to view cloud log. <br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml batch_predict --cloud\n",
    "model: $full_evaluation_model_name\n",
    "output: $batch_predict_output\n",
    "format: json\n",
    "prediction_data:\n",
    "  csv: $eval_csv_file\n",
    "cloud_config:\n",
    "  job_id: mlworkbench_batch_prediction_job_name_3\n",
    "  region: us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 B  2017-07-26T00:08:16Z  gs://cloud-ml-dev-mlworkbench-20news-lab/batch_predict_output/prediction.errors_stats-00000-of-00001\r\n",
      "  2.27 MiB  2017-07-26T00:07:54Z  gs://cloud-ml-dev-mlworkbench-20news-lab/batch_predict_output/prediction.results-00000-of-00001\r\n",
      "TOTAL: 2 objects, 2383983 bytes (2.27 MiB)\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -lh $batch_predict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"target\": \"rec.autos\", \"probability\": 0.35624340176582336, \"probability_4\": 0.06863237172365189, \"predicted\": \"rec.autos\", \"probability_3\": 0.07430016994476318, \"probability_2\": 0.1993453949689865, \"predicted_2\": \"comp.sys.mac.hardware\", \"predicted_3\": \"rec.sport.baseball\", \"predicted_4\": \"soc.religion.christian\"}\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat $batch_predict_output/prediction.results* | head -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Instant prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction within MLWorkbench\n",
    "\n",
    "The MLWorkbench also supports running prediction on the deployed model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted</th>\n",
       "      <th>predicted_2</th>\n",
       "      <th>predicted_3</th>\n",
       "      <th>predicted_4</th>\n",
       "      <th>probability</th>\n",
       "      <th>probability_2</th>\n",
       "      <th>probability_3</th>\n",
       "      <th>probability_4</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sci.space</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>0.129927</td>\n",
       "      <td>0.069090</td>\n",
       "      <td>0.068524</td>\n",
       "      <td>0.060403</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>0.200934</td>\n",
       "      <td>0.064302</td>\n",
       "      <td>0.064078</td>\n",
       "      <td>0.062777</td>\n",
       "      <td>windows xp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ml predict --cloud\n",
    "model: $full_regular_model_name\n",
    "headers: text\n",
    "prediction_data:\n",
    "  - nasa\n",
    "  - windows xp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from a python client\n",
    "\n",
    "See the previous notebook in this sequence for the example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Clean up\n",
    "\n",
    "This section is optional. We will delete all the GCP resources and local files created in this sequence of notebooks. If you are not ready to delete anything, don't run any of the following cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the deployed versions and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation \"projects/cloud-ml-dev/operations/delete_datalab_mlworkbench_20news_model_evaluation_version-1501027934074\"\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ModelVersions(mlengine_model_name).delete(mlengine_evaluation_version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation \"projects/cloud-ml-dev/operations/delete_datalab_mlworkbench_20news_model_v1-1501027965090\"\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ModelVersions(mlengine_model_name).delete(mlengine_regular_version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation \"projects/cloud-ml-dev/operations/delete_model_datalab_mlworkbench_20news_model-1501027983\"\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "Models().delete(mlengine_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the files in the GCS bucket, and delete the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/news_clean_test.csv#1501025712935123...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/news_clean_train.csv#1501025713034551...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/analyze_output/features.json#1501025749387412...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/analyze_output/stats.json#1501025747335926...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/analyze_output/vocab_news_label.csv#1501025735943598...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/batch_predict_output/prediction.errors_stats-00000-of-00001#1501027696147452...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/#1501026698479947...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/batch_predict_output/prediction.results-00000-of-00001#1501027674964638...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/analyze_output/schema.json#1501025748239368...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/analyze_output/vocab_text.csv#1501025742937245...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/variables/#1501026702691316...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/saved_model.pb#1501026699424861...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/assets.extra/features.json#1501026701098490...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/assets.extra/schema.json#1501026701760825...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/assets.extra/#1501026700248634...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/variables/variables.data-00000-of-00001#1501026703710888...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/#1501026679712914...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/assets.extra/#1501026677474110...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/assets.extra/features.json#1501026682410141...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/variables/variables.index#1501026675264479...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/saved_model.pb#1501026680715270...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/evaluation_model/variables/variables.index#1501026704330133...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-2786.data-00000-of-00003#1501026621545047...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/variables/variables.data-00000-of-00001#1501026684940615...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/variables/variables.index#1501026685711785...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/#1501026653979388...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/schema_without_target.json#1501026545387893...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/eval/#1501026563920464...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/staging/trainer.tar.gz#1501026262231516...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/checkpoint#1501026659105756...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/eval/events.out.tfevents.1501026564.master-3ce73c6343-0-nth67#1501026635708918...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/events.out.tfevents.1501026506.master-3ce73c6343-0-nth67#1501026661786470...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/assets.extra/#1501026696162698...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/#1501026688465689...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/eval/events.out.tfevents.1501026665.master-3ce73c6343-0-nth67#1501026666657013...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/#1501026688855719...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/saved_model.pb#1501026695560106...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/#1501026669296206...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/variables/#1501026692735936...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/variables/variables.index#1501026694079600...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/variables/variables.data-00000-of-00001#1501026693589351...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/assets.extra/schema.json#1501026696737841...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_evaluation_models/1501026687/assets.extra/features.json#1501026697623989...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/#1501026669741998...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/variables/#1501026673782591...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/assets.extra/schema.json#1501026678118716...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/variables/variables.data-00000-of-00001#1501026674550599...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/assets.extra/features.json#1501026678908874...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/assets.extra/schema.json#1501026683052664...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/#1501026670053358...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-11.data-00002-of-00003#1501026553899466...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-11.meta#1501026559175704...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/export/intermediate_prediction_models/1501026668/saved_model.pb#1501026676903763...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/assets.extra/#1501026681491942...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/model/variables/#1501026683983567...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-11.data-00001-of-00003#1501026554318844...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-11.data-00000-of-00003#1501026554881303...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-11.index#1501026555410429...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/graph.pbtxt#1501026548239173...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-2786.index#1501026622403882...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-2786.data-00002-of-00003#1501026620306662...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-2786.data-00001-of-00003#1501026620873486...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-2786.meta#1501026626694563...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-5006.data-00000-of-00003#1501026656491519...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-5006.data-00001-of-00003#1501026655133185...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-5006.data-00002-of-00003#1501026655924787...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-5006.index#1501026657241256...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/training_output/train/model.ckpt-5006.meta#1501026661090014...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/errors_features_eval-00000-of-00001.txt#1501026237429401...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/errors_features_train-00000-of-00001.txt#1501026078010907...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00000-of-00008.tfrecord.gz#1501026238299765...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00003-of-00008.tfrecord.gz#1501026238286780...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00002-of-00008.tfrecord.gz#1501026238291579...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00001-of-00008.tfrecord.gz#1501026238241946...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00005-of-00008.tfrecord.gz#1501026238301849...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00007-of-00008.tfrecord.gz#1501026238340630...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00006-of-00008.tfrecord.gz#1501026238364815...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_eval-00004-of-00008.tfrecord.gz#1501026238321882...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00000-of-00006.tfrecord.gz#1501026081739656...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00001-of-00006.tfrecord.gz#1501026081717513...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00002-of-00006.tfrecord.gz#1501026081938336...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00003-of-00006.tfrecord.gz#1501026081722843...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00004-of-00006.tfrecord.gz#1501026081846996...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/features_train-00005-of-00006.tfrecord.gz#1501026081766684...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/tmp/dataflow-job-20170725233601.1501025765.739490/dataflow_python_sdk.tar#1501025770651015...\n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/transform_output/tmp/dataflow-job-20170725233601.1501025765.739490/workflow.tar.gz#1501025766743217...\n",
      "/ [86/86 objects] 100% Done                                                     \n",
      "Operation completed over 86 objects.                                             \n",
      "Removing gs://cloud-ml-dev-mlworkbench-20news-lab/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rm -r $gcs_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the local files from the previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -fr $WORKSPACE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
