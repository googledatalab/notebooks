{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Airflow Setup.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"3WSgPBsGt2aW","colab_type":"text"},"source":["# Setup"],"cell_type":"markdown"},{"metadata":{"id":"v4UMnOJJng1s","colab_type":"text"},"source":["This notebook creates a VM in the user's project with the airflow scheduler and webserver. A default GCP zone for the VM has been chosen (below). Feel free to change this as desired."],"cell_type":"markdown"},{"metadata":{"id":"cFjQUmz5jIRT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["zone='us-central1-b'"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"ppy40Vihk1xk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"3ee44a62-9223-4de3-e40e-a87d245cfcc2","executionInfo":{"status":"ok","timestamp":1517014477940,"user_tz":480,"elapsed":3430,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["from google.datalab import Context\n","import google.datalab.storage as storage\n","\n","project = Context.default().project_id\n","vm_name = 'datalab-airflow'\n","\n","# The name of this GCS bucket follows a convention between this notebook and \n","# the 'BigQuery Pipeline' tutorial notebook, so don't change this.\n","gcs_dag_bucket_name = project + '-' + vm_name\n","gcs_dag_bucket = storage.Bucket(gcs_dag_bucket_name)\n","gcs_dag_bucket.create()"],"cell_type":"code","execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Google Cloud Storage Bucket gs://cloud-ml-dev-datalab-airflow-1"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"9HivgAXasXyN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":68},"outputId":"93d79988-13a2-4756-a35d-99974824eb8e","executionInfo":{"status":"ok","timestamp":1517014538915,"user_tz":480,"elapsed":60966,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["vm_startup_script_contents = \"\"\"#!/bin/bash\n","apt-get update\n","apt-get --assume-yes install python-pip\n","\n","DATALAB_TAR=datalab-1.1.0.tar\n","gsutil cp gs://datalab-pipelines/$DATALAB_TAR $DATALAB_TAR\n","pip install $DATALAB_TAR\n","rm $DATALAB_TAR\n","\n","pip install apache-airflow==1.9.0\n","pip install pandas-gbq==0.3.0\n","\n","export AIRFLOW_HOME=/airflow\n","export AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n","export AIRFLOW__CORE__LOAD_EXAMPLES=False\n","airflow initdb\n","airflow scheduler &\n","airflow webserver -p 8080 &\n","\n","# We append a gsutil rsync command to the cron file and have this run every minute to sync dags.\n","PROJECT_ID=$(gcloud info --format=\"get(config.project)\")\n","GCS_DAG_BUCKET=$PROJECT_ID-datalab-airflow\n","AIRFLOW_CRON=temp_crontab.txt\n","crontab -l > $AIRFLOW_CRON\n","DAG_FOLDER=\"dags\"\n","LOCAL_DAG_PATH=$AIRFLOW_HOME/$DAG_FOLDER\n","mkdir $LOCAL_DAG_PATH\n","echo \"* * * * * gsutil rsync gs://$GCS_DAG_BUCKET/$DAG_FOLDER $LOCAL_DAG_PATH\" >> $AIRFLOW_CRON\n","crontab $AIRFLOW_CRON\n","rm $AIRFLOW_CRON\n","EOF\n","\"\"\"\n","vm_startup_script_file_name = 'vm_startup_script.sh'\n","script_file = open(vm_startup_script_file_name, 'w')\n","script_file.write(vm_startup_script_contents)\n","script_file.close()\n","import subprocess\n","print subprocess.check_output([\n","    'gcloud', 'compute', '--project', project, 'instances', 'create', vm_name, \n","    '--zone', zone,\n","    '--machine-type', 'n1-standard-1',\n","    '--network', 'default',\n","    '--maintenance-policy', 'MIGRATE',\n","    '--scopes', 'https://www.googleapis.com/auth/cloud-platform',\n","    '--image', 'debian-9-stretch-v20171025',\n","    '--min-cpu-platform', 'Automatic',\n","    '--image-project', 'debian-cloud',\n","    '--boot-disk-size', '10',\n","    '--boot-disk-type', 'pd-standard',\n","    '--boot-disk-device-name', vm_name,\n","    '--metadata-from-file', 'startup-script=' + vm_startup_script_file_name])"],"cell_type":"code","execution_count":9,"outputs":[{"output_type":"stream","text":["NAME               ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS\n","datalab-airflow-1  us-central1-b  n1-standard-1               10.240.0.14  35.192.219.140  RUNNING\n","\n"],"name":"stdout"}]},{"metadata":{"id":"p9QSZ9J5twi0","colab_type":"text"},"source":["# Cleanup\n"],"cell_type":"markdown"},{"metadata":{"id":"HtswWJqttq99","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["#The following cleans up the VM and associated GCS bucket. Uncomment and run.\n","#!gsutil rm -r gs://$gcs_dag_bucket_name\n","#!gsutil rb gs://$gcs_dag_bucket_name\n","#!gcloud compute instances delete datalab-airflow --zone us-central1-b --quiet"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"tg1Hl1ivH2yb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# This just verifies that cleanup actually worked. Should show an error like \n","# \"BucketNotFoundException: 404 ...\". \n","!gsutil ls gs://$gcs_dag_bucket_name"],"cell_type":"code","execution_count":0,"outputs":[]}]}