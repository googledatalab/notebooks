{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a VM in the user's project with the airflow scheduler and webserver. A default GCP zone for the VM has been chosen (below). Feel free to change this as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WSgPBsGt2aW"
   },
   "source": [
    "# Setup\n",
    "\n",
    "From a terminal (especially if you're not using Google Cloud Datalab for this notebook), execute:\n",
    "> gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cFjQUmz5jIRT"
   },
   "outputs": [],
   "source": [
    "zone='us-central1-b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3534,
     "status": "ok",
     "timestamp": 1517530931370,
     "user": {
      "displayName": "Rajiv Bharadwaja",
      "userId": "114213070630621217486"
     },
     "user_tz": 480
    },
    "id": "ppy40Vihk1xk",
    "outputId": "bd0bb7ad-50cf-403e-f256-fe5799e23f21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Google Cloud Storage Bucket gs://cloud-ml-dev-datalab-airflow-11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab import Context\n",
    "import google.datalab.storage as storage\n",
    "\n",
    "project = Context.default().project_id\n",
    "vm_name = 'datalab-airflow'\n",
    "\n",
    "# The name of this GCS bucket follows a convention between this notebook and \n",
    "# the 'BigQuery Pipeline' tutorial notebook, so don't change this.\n",
    "gcs_dag_bucket_name = project + '-' + vm_name\n",
    "gcs_dag_bucket = storage.Bucket(gcs_dag_bucket_name)\n",
    "gcs_dag_bucket.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25184,
     "status": "ok",
     "timestamp": 1517530956567,
     "user": {
      "displayName": "Rajiv Bharadwaja",
      "userId": "114213070630621217486"
     },
     "user_tz": 480
    },
    "id": "9HivgAXasXyN",
    "outputId": "5204ad0e-748f-4dab-88bf-11bcf3b700b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP      STATUS\n",
      "datalab-airflow-11  us-central1-b  n1-standard-1               10.240.0.7   104.154.134.180  RUNNING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm_startup_script_contents = \"\"\"#!/bin/bash\n",
    "apt-get update\n",
    "apt-get --assume-yes install python-pip\n",
    "\n",
    "pip install datalab==1.1.2\n",
    "pip install apache-airflow==1.9.0\n",
    "pip install pandas-gbq==0.3.0\n",
    "\n",
    "export AIRFLOW_HOME=/airflow\n",
    "export AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n",
    "export AIRFLOW__CORE__LOAD_EXAMPLES=False\n",
    "airflow initdb\n",
    "airflow scheduler &\n",
    "airflow webserver -p 8080 &d\n",
    "\n",
    "# We append a gsutil rsync command to the cron file and have this run every minute to sync dags.\n",
    "PROJECT_ID=$(gcloud info --format=\"get(config.project)\")\n",
    "GCS_DAG_BUCKET=$PROJECT_ID-datalab-airflow\n",
    "AIRFLOW_CRON=temp_crontab.txt\n",
    "crontab -l > $AIRFLOW_CRON\n",
    "DAG_FOLDER=\"dags\"\n",
    "LOCAL_DAG_PATH=$AIRFLOW_HOME/$DAG_FOLDER\n",
    "mkdir $LOCAL_DAG_PATH\n",
    "echo \"* * * * * gsutil rsync gs://$GCS_DAG_BUCKET/$DAG_FOLDER $LOCAL_DAG_PATH\" >> $AIRFLOW_CRON\n",
    "crontab $AIRFLOW_CRON\n",
    "rm $AIRFLOW_CRON\n",
    "EOF\n",
    "\"\"\"\n",
    "vm_startup_script_file_name = 'vm_startup_script.sh'\n",
    "script_file = open(vm_startup_script_file_name, 'w')\n",
    "script_file.write(vm_startup_script_contents)\n",
    "script_file.close()\n",
    "import subprocess\n",
    "print subprocess.check_output([\n",
    "    'gcloud', 'compute', '--project', project, 'instances', 'create', vm_name, \n",
    "    '--zone', zone,\n",
    "    '--machine-type', 'n1-standard-1',\n",
    "    '--network', 'default',\n",
    "    '--maintenance-policy', 'MIGRATE',\n",
    "    '--scopes', 'https://www.googleapis.com/auth/cloud-platform',\n",
    "    '--image', 'debian-9-stretch-v20171025',\n",
    "    '--min-cpu-platform', 'Automatic',\n",
    "    '--image-project', 'debian-cloud',\n",
    "    '--boot-disk-size', '10',\n",
    "    '--boot-disk-type', 'pd-standard',\n",
    "    '--boot-disk-device-name', vm_name,\n",
    "    '--metadata-from-file', 'startup-script=' + vm_startup_script_file_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9QSZ9J5twi0"
   },
   "source": [
    "# Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HtswWJqttq99"
   },
   "outputs": [],
   "source": [
    "#The following cleans up the VM and associated GCS bucket. Uncomment and run.\n",
    "#!gsutil rm -r gs://$gcs_dag_bucket_name\n",
    "#!gcloud compute instances delete datalab-airflow --zone us-central1-b --quiet\n",
    "\n",
    "# This just verifies that cleanup actually worked. Uncomment and run. Should \n",
    "# show an error like \"BucketNotFoundException: 404 ...\". \n",
    "# !gsutil ls gs://$gcs_dag_bucket_name"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Airflow Setup.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
