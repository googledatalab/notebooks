{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WSgPBsGt2aW"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v4UMnOJJng1s"
   },
   "source": [
    "This notebook creates a VM in the user's project with the airflow scheduler and webserver. A default GCP zone for the VM has been chosen (below). Feel free to change this as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cFjQUmz5jIRT"
   },
   "outputs": [],
   "source": [
    "zone='us-central1-b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ppy40Vihk1xk"
   },
   "outputs": [],
   "source": [
    "from google.datalab import Context\n",
    "import google.datalab.storage as storage\n",
    "\n",
    "project = Context.default().project_id\n",
    "vm_name = 'datalab-airflow'\n",
    "\n",
    "# The name of this GCS bucket follows a convention between this notebook and \n",
    "# the 'BigQuery Pipeline' tutorial notebook, so don't change this.\n",
    "gcs_dag_bucket_name = project + '-' + vm_name\n",
    "gcs_dag_bucket = storage.Bucket(gcs_dag_bucket_name)\n",
    "gcs_dag_bucket.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9HivgAXasXyN"
   },
   "outputs": [],
   "source": [
    "vm_startup_script_contents = \"\"\"#!/bin/bash\n",
    "apt-get update\n",
    "apt-get --assume-yes install python-pip\n",
    "\n",
    "DATALAB_TAR=datalab-1.1.0.tar\n",
    "gsutil cp gs://datalab-pipelines/$DATALAB_TAR $DATALAB_TAR\n",
    "pip install $DATALAB_TAR\n",
    "rm $DATALAB_TAR\n",
    "\n",
    "pip install apache-airflow==1.9.0\n",
    "pip install pandas-gbq==0.3.0\n",
    "\n",
    "export AIRFLOW_HOME=/airflow\n",
    "export AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n",
    "export AIRFLOW__CORE__LOAD_EXAMPLES=False\n",
    "airflow initdb\n",
    "airflow scheduler &\n",
    "airflow webserver -p 8080 &\n",
    "\n",
    "# We append a gsutil rsync command to the cron file and have this run every minute to sync dags.\n",
    "PROJECT_ID=$(gcloud info --format=\"get(config.project)\")\n",
    "GCS_DAG_BUCKET=$PROJECT_ID-datalab-airflow\n",
    "AIRFLOW_CRON=temp_crontab.txt\n",
    "crontab -l > $AIRFLOW_CRON\n",
    "DAG_FOLDER=\"dags\"\n",
    "LOCAL_DAG_PATH=$AIRFLOW_HOME/$DAG_FOLDER\n",
    "mkdir $LOCAL_DAG_PATH\n",
    "echo \"* * * * * gsutil rsync gs://$GCS_DAG_BUCKET/$DAG_FOLDER $LOCAL_DAG_PATH\" >> $AIRFLOW_CRON\n",
    "crontab $AIRFLOW_CRON\n",
    "rm $AIRFLOW_CRON\n",
    "EOF\n",
    "\"\"\"\n",
    "vm_startup_script_file_name = 'vm_startup_script.sh'\n",
    "script_file = open(vm_startup_script_file_name, 'w')\n",
    "script_file.write(vm_startup_script_contents)\n",
    "script_file.close()\n",
    "import subprocess\n",
    "print subprocess.check_output([\n",
    "    'gcloud', 'compute', '--project', project, 'instances', 'create', vm_name, \n",
    "    '--zone', zone,\n",
    "    '--machine-type', 'n1-standard-1',\n",
    "    '--network', 'default',\n",
    "    '--maintenance-policy', 'MIGRATE',\n",
    "    '--scopes', 'https://www.googleapis.com/auth/cloud-platform',\n",
    "    '--image', 'debian-9-stretch-v20171025',\n",
    "    '--min-cpu-platform', 'Automatic',\n",
    "    '--image-project', 'debian-cloud',\n",
    "    '--boot-disk-size', '10',\n",
    "    '--boot-disk-type', 'pd-standard',\n",
    "    '--boot-disk-device-name', vm_name,\n",
    "    '--metadata-from-file', 'startup-script=' + vm_startup_script_file_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9QSZ9J5twi0"
   },
   "source": [
    "# Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HtswWJqttq99"
   },
   "outputs": [],
   "source": [
    "#The following cleans up the VM and associated GCS bucket. Uncomment and run.\n",
    "#!gsutil rm -r gs://$gcs_dag_bucket_name\n",
    "#!gcloud compute instances delete datalab-airflow --zone us-central1-b --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tg1Hl1ivH2yb"
   },
   "outputs": [],
   "source": [
    "# This just verifies that cleanup actually worked. Should show an error like \n",
    "# \"BucketNotFoundException: 404 ...\". \n",
    "!gsutil ls gs://$gcs_dag_bucket_name"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Airflow Setup.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
