{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BigQuery Pipeline.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"ESTHNsH4TxDd","colab_type":"text"},"source":["# BigQuery Pipeline\n","Google Cloud Datalab, with the *pipeline* subcommand, enables productionizing (i.e. scheduling and orchestrating) notebooks that accomplish ETL with BigQuery and GCS. \n","\n","Disclaimer: This is still in the experimental stage.\n"],"cell_type":"markdown"},{"metadata":{"id":"_sabjhoUVGkG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%bq pipeline --help"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"y3cVwcERjBAf","colab_type":"text"},"source":["# Setup\n","We set up a few tutorial-essentials in the following cell. These are garbage-collected in the *Cleanup* cell at the end of this notebook.\n","\n","\n","We use Apache Airflow (https://airflow.apache.org/start.html) as the underlying technology for orchestrating and scheduling. To set this up, please first run the \"Airflow Setup\" notebook (under samples/contrib/pipeline/ at https://datalab-alpha.cloud.google.com/docs); it will setup a GCE VM with the Airflow scheduler and the webserver as a long-running process.\n","\n","Running all the cells in the notebook would ensure that there is a VM instance named \"datalab-airflow\" in your project. Note: Without the correct setup of the VM above, the *pipeline* subcommand in the cells below will not work (pipelines will not be deployed)."],"cell_type":"markdown"},{"metadata":{"id":"g1FpZfp7jAhV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["from google.datalab import Context\n","import datetime\n","import google.datalab.bigquery as bq\n","import google.datalab.kernel\n","import google.datalab.storage as storage\n","\n","# We import the requisite moduled from contrib\n","try:\n","  import google.datalab.contrib.bigquery.commands\n","  import google.datalab.contrib.pipeline.airflow\n","except ImportError:\n","  print('Please install the latest pydatalab with the following command:')\n","  print('pip install --upgrade --force-reinstall datalab')\n","  \n","# We use the project-id to uniquify GCP resource names\n","project = Context.default().project_id\n","bucket_name = project + '-bq_pipeline'\n","bucket = storage.Bucket(bucket_name)\n","bucket.create()\n","print(bucket.exists())\n","\n","dataset_name = 'bq_pipeline'\n","dataset = bq.Dataset(dataset_name)\n","dataset.create()\n","print(dataset.exists())\n","\n","vm_name = 'datalab-airflow'\n","gcs_dag_bucket_name = project + '-' + vm_name\n","gcs_dag_file_path = 'dags'\n","\n","# Start and end timestamps for our pipelines. \n","start = datetime.datetime.now()\n","formatted_start = start.strftime('%Y%m%dT%H%M%S')\n","end = start + datetime.timedelta(minutes=5)\n"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"VMkRPlgIW783","colab_type":"text"},"source":["# Building a data transformation pipeline\n","The *pipeline* subcommand deploys and orchestrates an ETL pipeline. It supports specifying either an existing BQ table or a GCS path (with accompanying schema) as the data *input*, executing a *transformation* with BQ SQL and producing an *output* of the results (again, either a BQ table or a GCS path). This *pipeline* can be executed on a *schedule*. Additionally, *parameters* can be specified to templatize or customize the pipeline."],"cell_type":"markdown"},{"metadata":{"id":"GtQenup-VZR8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["github_archive = 'githubarchive.month.201801'"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"uWESJgLIMuXm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq query --name my_pull_request_events\n","SELECT id, created_at, repo.name FROM input\n","WHERE actor.login = 'rajivpb' AND type = 'PullRequestEvent'"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"6GdKyfdNM-ti","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# We designate the following 'output' for our pipeline. \n","results_table = project + '.' + dataset_name + '.' + 'pr_events_' + formatted_start\n","\n","# Pipeline name is made unique by suffixing a timestamp\n","pipeline_name = 'github_once_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"eIM5dQm53U9k","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","input:\n","  table: $github_archive\n","transformation:\n","  query: my_pull_request_events\n","output:\n","  table: $results_table\n","  mode: overwrite\n","schedule:\n","  start: $start\n","  end: $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"ezMr2phDJZ_T","colab_type":"text"},"source":["When the above cell is run, a pipeline is deployed and the results of the query are written into the BQ results table (i.e. $results_table). It could take 5-10 min between when the cell is executed for the result_table to show up. Below, we'll see additional examples for alternate ways of specifying the source, the source-types supported, and for customizing the pipeline."],"cell_type":"markdown"},{"metadata":{"id":"qE1wOSrsMQbB","colab_type":"text"},"source":["# Parameterization\n","The *parameters* section provides the ability to customize the inputs and outputs of the pipeline. These parameters are merged with the SQL query parameters into a list, and are specified in the cell body (along the same lines as the *%bq execute* command, for example). \n","\n","In addition to parameters that the users can define, the following mapping keys have been made available for formatting strings and are designed to capture common scenarios around parameterizing the pipeline with the execution timestamp. \n","\n"," - '_ds': the date formatted as YYYY-MM-DD\n"," - '_ts': the full ISO-formatted timestamp YYYY-MM-DDTHH:MM:SS.mmmmmm\n"," - '_ds_nodash': the date formatted as YYYYMMDD (i.e. YYYY-MM-DD with 'no dashes')\n"," - '_ts_nodash': the timestamp formatted as YYYYMMDDTHHMMSSmmmmmm (i.e full ISO-formatted timestamp without dashes or colons)\n"," - '_ts_year': 4-digit year\n"," - '_ts_month': '1'-'12'\n"," - '_ts_day': '1'-'31'\n"," - '_ts_hour': '0'-'23'\n"," - '_ts_minute': '0'-'59'\n"," - '_ts_second': '0'-'59'\n"],"cell_type":"markdown"},{"metadata":{"id":"jLUJaAzbpyuQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# The source/input is formatted with the built-in mapping keys _ts_year and \n","# _ts_month and these are evaluated (or \"bound\") at the time of pipeline \n","# execution. This could be at some point in the future, or at some point in the \n","# \"past\" in cases where a backfill job is being executed.\n","github_archive_current_month = 'githubarchive.month.%(_ts_year)s01'\n","\n","# The destination/output is formatted with additional user-defined parameters\n","# 'project', 'dataset', and 'user'. These are evaluated/bound at the time of \n","# execution of the %bq pipeline cell. \n","results_table = '%(project)s.%(dataset_name)s.%(user)s_pr_events_%(_ts_nodash)s'\n","\n","pipeline_name = 'github_parameterized_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"OeAJ6o6swS78","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq query --name my_pull_request_events\n","SELECT id, created_at, repo.name FROM input\n","WHERE actor.login = @user AND type = 'PullRequestEvent'\n"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"-_3F4BdjqQzz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","input:\n","  table: $github_archive_current_month\n","transformation:\n","  query: my_pull_request_events\n","output:\n","  table: $results_table\n","  mode: overwrite\n","parameters:\n","  - name: user\n","    type: STRING\n","    value: 'rajivpb'\n","  - name: project\n","    type: STRING\n","    value: $project\n","  - name: dataset_name\n","    type: STRING\n","    value: $dataset_name\n","schedule:\n","  start: $start\n","  end: $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"81_9F7vqe7Hm","colab_type":"text"},"source":["# SQL-based data transformation pipeline for GCS data\n","*pipeline* also supports specifying GCS paths as both the input (accompanied by a schema) and output, thus completely bypassing the specification of any BQ tables. Garbage collection of all intermediate BQ tables is handled for the user."],"cell_type":"markdown"},{"metadata":{"id":"m8Kx_Unj-E1V","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["gcs_input_path = 'gs://cloud-datalab-samples/cars.csv'\n","gcs_output_path = 'gs://%(bucket_name)s/all_makes_%(_ts_nodash)s.csv'\n","pipeline_name = 'gcs_to_gcs_transform_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"11R6SnbSOokH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq query --name all_makes\n","SELECT Make FROM input"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"xqGHWFlLex_V","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","input:\n","  path: $gcs_input_path\n","  schema:\n","    - name: Year\n","      type: INTEGER\n","    - name: Make\n","      type: STRING\n","    - name: Model\n","      type: STRING\n","    - name: Description\n","      type: STRING\n","    - name: Price\n","      type: FLOAT\n","  csv:\n","    skip: 1\n","transformation: \n","  query: all_makes\n","output:\n","  path: $gcs_output_path\n","parameters:\n","  - name: bucket_name\n","    type: STRING\n","    value: $bucket_name\n","schedule:\n","  start: $start\n","  end:  $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"16QvAy_WSyDn","colab_type":"text"},"source":["# Load data from GCS into BigQuery\n","*pipeline* can also be used to parameterize and schedule the loading of data from GCS to BQ, i.e the equivalent of the *%bq load* command."],"cell_type":"markdown"},{"metadata":{"id":"RVwTNF4jnGA7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["bq_load_results_table = '%(project)s.%(dataset_name)s.cars_load'\n","pipeline_name = 'load_gcs_to_bq_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"-qHQ4bfkXwEU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","load:\n","  path: $gcs_input_path\n","  schema:\n","    - name: Year\n","      type: INTEGER\n","    - name: Make\n","      type: STRING\n","    - name: Model\n","      type: STRING\n","    - name: Description\n","      type: STRING\n","    - name: Price\n","      type: FLOAT\n","  csv:\n","    skip: 1\n","  table: $bq_load_results_table\n","  mode: overwrite\n","parameters:\n","  - name: project\n","    type: STRING\n","    value: $project\n","  - name: dataset_name\n","    type: STRING\n","    value: $dataset_name\n","schedule:\n","  start: $start\n","  end: $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"cnuNrYDCXiXT","colab_type":"text"},"source":["# Extract data from BigQuery into GCS\n","Similar to load, *pipeline* can also be used to perform the equivalent of the *%bq extract* command. To illustrate, we extract the data in the table that was the result of the 'load' pipeline, and write it to a GCS file. \n","\n","Now, it's possible that if you \"Ran All Cells\" in this notebook, this pipeline gets deployed at the same time as the previous load-pipeline, in which case the source table isn't yet ready. Hence we set *retries* to 3, with a delay of 90 seconds and hope that the table eventually does get created and this pipeline is successful."],"cell_type":"markdown"},{"metadata":{"id":"nWgnLdX9YH7e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["gcs_extract_path = 'gs://%(bucket_name)s/cars_extract_%(_ts_nodash)s.csv'\n","pipeline_name = 'extract_bq_to_gcs_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"uu1YuNJIFpTM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","extract:\n","  table: $bq_load_results_table\n","  path: $gcs_extract_path\n","  format: csv\n","  csv:\n","    delimiter: '#'\n","parameters:\n","  - name: bucket_name\n","    type: STRING\n","    value: $bucket_name\n","  - name: project\n","    type: STRING\n","    value: $project\n","  - name: dataset_name\n","    type: STRING\n","    value: $dataset_name\n","schedule:\n","  start: $start\n","  interval: '@once'\n","  catchup: True\n","  retries: 3\n","  retry_delay_seconds: 90"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"gjknsf6HIbJ_","colab_type":"text"},"source":["# Output of successful pipeline runs"],"cell_type":"markdown"},{"metadata":{"id":"7N-r2h5bK9ml","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# You will see two files named all_makes_<timestamp> and cars_extract_<timestamp>\n","# under the bucket:\n","!gsutil ls gs://$bucket_name\n","  \n","# You will see three tables named cars_load, pr_events_<timestamp> and \n","# <user>_pr_events_<timestamp> under the BigQuery dataset:\n","!bq ls $dataset_name"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"kyph2e8WhlCf","colab_type":"text"},"source":["#Cleanup"],"cell_type":"markdown"},{"metadata":{"id":"G7MApPqchoul","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# Delete the contents of the GCS bucket, the GCS bucket itself, and the BQ \n","# dataset. Uncomment the lines below and execute.\n","#!gsutil rm -r gs://$bucket_name\n","#!bq rm -r -f $dataset_name"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"kCJPb1RVQbjq","colab_type":"text"},"source":["# Stop Billing"],"cell_type":"markdown"},{"metadata":{"id":"lutR46eiQb_6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# This deletes the long-running VM with the Airflow scheduler. Uncomment the \n","# line below and execute.\n","#!gcloud compute instances stop $vm_name --zone us-central1-b --quiet"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"YU5SK7_lj7_S","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# This just verifies that cleanup actually worked. Run this after running the \n","# 'Cleanup' cell\n","\n","#Should show two error messages like \"BucketNotFoundException: 404 gs://...\"\n","!gsutil ls gs://$bucket_name\n","!gsutil ls gs://$gcs_dag_bucket_name/dags\n","  \n","#Should show an error message like \"BigQuery error in ls operation: Not found ...\"\n","!bq ls $dataset_name\n"],"cell_type":"code","execution_count":0,"outputs":[]}]}