{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESTHNsH4TxDd"
   },
   "source": [
    "# BigQuery Pipeline\n",
    "Google Cloud Datalab, with the *pipeline* subcommand, enables productionizing (i.e. scheduling and orchestrating) notebooks that accomplish ETL with BigQuery and GCS. It uses Apache Airflow (https://airflow.apache.org/start.html) as the underlying technology for orchestrating and scheduling. \n",
    "\n",
    "Disclaimer: This is still in the experimental stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3cVwcERjBAf"
   },
   "source": [
    "# Setup\n",
    "\n",
    "## Google Cloud Composer\n",
    "Set up a Google Cloud Composer environment using the instructions here: https://cloud.google.com/composer/docs/quickstart, and specify 'datalab' as a python dependency using the instructions here: https://cloud.google.com/composer/docs/how-to/using/installing-python-dependencies. The examples in the cells below assume that a Composer environment is available. \n",
    "\n",
    "## Airflow\n",
    "Alternately, you could also set up your own VM with Airflow as a long-running process. Run the \"Airflow Setup\" notebook (under samples/contrib/pipeline/); it will setup a GCE VM with the Airflow Scheduler and the dashboard webserver. \n",
    "\n",
    "The *pipeline* subcommand in the cells below (and for the pipelines to be deployed successfully) needs either the Composer setup or the Airflow setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4293,
     "status": "ok",
     "timestamp": 1517531265056,
     "user": {
      "displayName": "Rajiv Bharadwaja",
      "userId": "114213070630621217486"
     },
     "user_tz": 480
    },
    "id": "g1FpZfp7jAhV",
    "outputId": "7a158185-4ace-4db3-8c1e-4aaea8ff579e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import google.datalab.bigquery as bq\n",
    "import google.datalab.contrib.bigquery.commands\n",
    "import google.datalab.contrib.pipeline.airflow\n",
    "import google.datalab.contrib.pipeline.composer\n",
    "import google.datalab.kernel\n",
    "import google.datalab.storage as storage\n",
    "from google.datalab import Context\n",
    "\n",
    "project = Context.default().project_id\n",
    "\n",
    "# Composer variables (change this as per your preference)\n",
    "environment = 'rajivpb-composer-next'\n",
    "location = 'us-central1'\n",
    "\n",
    "# Airflow setup variables\n",
    "vm_name = 'datalab-airflow'\n",
    "gcs_dag_bucket_name = project + '-' + vm_name\n",
    "gcs_dag_file_path = 'dags'\n",
    "\n",
    "# Setup GCS bucket and BQ datastes\n",
    "bucket_name = project + '-bq_pipeline'\n",
    "bucket = storage.Bucket(bucket_name)\n",
    "bucket.create()\n",
    "print(bucket.exists())\n",
    "dataset_name = 'bq_pipeline'\n",
    "dataset = bq.Dataset(dataset_name)\n",
    "dataset.create()\n",
    "print(dataset.exists())\n",
    "\n",
    "# Start and end timestamps for our pipelines. \n",
    "start = datetime.datetime.now()\n",
    "formatted_start = start.strftime('%Y%m%dT%H%M%S')\n",
    "end = start + datetime.timedelta(minutes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: %bq pipeline [-h] -n NAME [-d GCS_DAG_BUCKET] [-f GCS_DAG_FILE_PATH]\n",
      "                    [-e ENVIRONMENT] [-l LOCATION] [-g DEBUG]\n",
      "\n",
      "Creates a GCS/BigQuery ETL pipeline. The cell-body is specified as follows:\n",
      "  input:\n",
      "    table | path: <BQ table name or GCS path; both if path->table load is also required>\n",
      "    schema: <For syntax, refer '%%bq execute'>\n",
      "    format: {csv (default) | json}\n",
      "    csv: <This section is relevant only when 'format' is 'csv'>\n",
      "      delimiter: <The field delimiter to use; default is ','>\n",
      "      skip: <Number of rows at the top of a CSV file to skip; default is 0>\n",
      "      strict: <{True | False (default)}; whether to accept rows with missing trailing (or optional) columns>\n",
      "      quote: <Value used to quote data sections; default is '\"'>\n",
      "    mode: <{append (default) | overwrite}; applicable if path->table load>\n",
      "  transformation: <optional; when absent, a direct conversion is done from input (path|table) to output (table|path)>\n",
      "    query: <name of BQ query defined via \"%%bq query --name ...\">\n",
      "  output:\n",
      "    table | path: <BQ table name or GCS path; both if table->path extract is required>\n",
      "    mode: <{append | overwrite | create (default)}; applicable only when table is specified.\n",
      "    format: <{csv (default) | json}>\n",
      "    csv: <This section is relevant only when 'format' is 'csv'>\n",
      "      delimiter: <the field delimiter to use. Defaults to ','>\n",
      "      header: <{True (default) | False}; Whether to include an initial header line>\n",
      "      compress: <{True | False (default) }; Whether to compress the data on export>\n",
      "  schedule:\n",
      "    start: <formatted as '%%Y-%%m-%%dT%%H:%%M:%%S'; default is 'now'>\n",
      "    end:  <formatted as '%%Y-%%m-%%dT%%H:%%M:%%S'; default is 'forever'>\n",
      "    interval: <{@once (default) | @hourly | @daily | @weekly | @ monthly | @yearly | <cron ex>}>\n",
      "    catchup: <{True | False (default)}; when True, backfill is performed for start and end times.\n",
      "    retries: Number of attempts to run the pipeline; default is 0\n",
      "    retry_delay_seconds: Number of seconds to wait before retrying the task\n",
      "  emails: <comma separated list of emails to notify in case of retries, failures, etc.>\n",
      "  parameters: <For syntax, refer '%%bq execute'>\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -n NAME, --name NAME  BigQuery pipeline name\n",
      "  -d GCS_DAG_BUCKET, --gcs_dag_bucket GCS_DAG_BUCKET\n",
      "                        The Google Cloud Storage bucket for the Airflow dags.\n",
      "  -f GCS_DAG_FILE_PATH, --gcs_dag_file_path GCS_DAG_FILE_PATH\n",
      "                        The file path suffix for the Airflow dags.\n",
      "  -e ENVIRONMENT, --environment ENVIRONMENT\n",
      "                        The name of the Google Cloud Composer environment.\n",
      "  -l LOCATION, --location LOCATION\n",
      "                        The location of the Google Cloud Composer environment. Refer https://cloud.google.com/about/locations/ for further details.\n",
      "  -g DEBUG, --debug DEBUG\n",
      "                        Debug output with the airflow spec.\n"
     ]
    }
   ],
   "source": [
    "%bq pipeline -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMkRPlgIW783"
   },
   "source": [
    "# Building a data transformation pipeline\n",
    "The *pipeline* subcommand deploys and orchestrates an ETL pipeline. It supports specifying either an existing BQ table or a GCS path (with accompanying schema) as the data *input*, executing a *transformation* with BQ SQL and producing an *output* of the results (again, either a BQ table or a GCS path). This *pipeline* can be executed on a *schedule*. Additionally, *parameters* can be specified to templatize or customize the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GtQenup-VZR8"
   },
   "outputs": [],
   "source": [
    "github_archive = 'githubarchive.month.201802'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uWESJgLIMuXm"
   },
   "outputs": [],
   "source": [
    "%%bq query --name my_pull_request_events\n",
    "SELECT id, created_at, repo.name FROM input\n",
    "WHERE actor.login = 'rajivpb' AND type = 'PullRequestEvent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6GdKyfdNM-ti"
   },
   "outputs": [],
   "source": [
    "# We designate the following 'output' for our pipeline. \n",
    "results_table = project + '.' + dataset_name + '.' + 'pr_events_' + formatted_start\n",
    "\n",
    "# Pipeline name is made unique by suffixing a timestamp\n",
    "pipeline_name = 'github_once_' + formatted_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eIM5dQm53U9k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Composer pipeline successfully deployed! View dashboard for more details.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq pipeline --name $pipeline_name -e $environment -l $location\n",
    "input:\n",
    "  table: $github_archive\n",
    "transformation:\n",
    "  query: my_pull_request_events\n",
    "output:\n",
    "  table: $results_table\n",
    "  mode: overwrite\n",
    "schedule:\n",
    "  start: $start\n",
    "  end: $end\n",
    "  interval: '@once'\n",
    "  catchup: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezMr2phDJZ_T"
   },
   "source": [
    "When the above cell is run, a pipeline is deployed and the results of the query are written into the BQ results table (i.e. $results_table). It could take 5-10 min between when the cell is executed for the result_table to show up. Below, we'll see additional examples for alternate ways of specifying the source, the source-types supported, and for customizing the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qE1wOSrsMQbB"
   },
   "source": [
    "# Parameterization\n",
    "The *parameters* section provides the ability to customize the inputs and outputs of the pipeline. These parameters are merged with the SQL query parameters into a list, and are specified in the cell body (along the same lines as the *%bq execute* command, for example). \n",
    "\n",
    "In addition to parameters that the users can define, the following mapping keys have been made available for formatting strings and are designed to capture common scenarios around parameterizing the pipeline with the execution timestamp. \n",
    "\n",
    " - '_ds': the date formatted as YYYY-MM-DD\n",
    " - '_ts': the full ISO-formatted timestamp YYYY-MM-DDTHH:MM:SS.mmmmmm\n",
    " - '_ds_nodash': the date formatted as YYYYMMDD (i.e. YYYY-MM-DD with 'no dashes')\n",
    " - '_ts_nodash': the timestamp formatted as YYYYMMDDTHHMMSSmmmmmm (i.e full ISO-formatted timestamp without dashes or colons)\n",
    " - '_ts_year': 4-digit year\n",
    " - '_ts_month': '1'-'12'\n",
    " - '_ts_day': '1'-'31'\n",
    " - '_ts_hour': '0'-'23'\n",
    " - '_ts_minute': '0'-'59'\n",
    " - '_ts_second': '0'-'59'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jLUJaAzbpyuQ"
   },
   "outputs": [],
   "source": [
    "# The source/input is formatted with the built-in mapping keys _ts_year and \n",
    "# _ts_month and these are evaluated (or \"bound\") at the time of pipeline \n",
    "# execution. This could be at some point in the future, or at some point in the \n",
    "# \"past\" in cases where a backfill job is being executed.\n",
    "github_archive_current_month = 'githubarchive.month.%(_ts_year)s%(_ts_month)s'\n",
    "\n",
    "# The destination/output is formatted with additional user-defined parameters\n",
    "# 'project', 'dataset', and 'user'. These are evaluated/bound at the time of \n",
    "# execution of the %bq pipeline cell. \n",
    "results_table = '%(project)s.%(dataset_name)s.%(user)s_pr_events_%(_ts_nodash)s'\n",
    "\n",
    "pipeline_name = 'github_parameterized_' + formatted_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OeAJ6o6swS78"
   },
   "outputs": [],
   "source": [
    "%%bq query --name my_pull_request_events\n",
    "SELECT id, created_at, repo.name FROM input\n",
    "WHERE actor.login = @user AND type = 'PullRequestEvent'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-_3F4BdjqQzz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Composer pipeline successfully deployed! View dashboard for more details.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq pipeline --name $pipeline_name -e $environment -l $location\n",
    "input:\n",
    "  table: $github_archive_current_month\n",
    "transformation:\n",
    "  query: my_pull_request_events\n",
    "output:\n",
    "  table: $results_table\n",
    "  mode: overwrite\n",
    "parameters:\n",
    "  - name: user\n",
    "    type: STRING\n",
    "    value: 'rajivpb'\n",
    "  - name: project\n",
    "    type: STRING\n",
    "    value: $project\n",
    "  - name: dataset_name\n",
    "    type: STRING\n",
    "    value: $dataset_name\n",
    "schedule:\n",
    "  start: $start\n",
    "  end: $end\n",
    "  interval: '@once'\n",
    "  catchup: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81_9F7vqe7Hm"
   },
   "source": [
    "# SQL-based data transformation pipeline for GCS data\n",
    "*pipeline* also supports specifying GCS paths as both the input (accompanied by a schema) and output, thus completely bypassing the specification of any BQ tables. Garbage collection of all intermediate BQ tables is handled for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "m8Kx_Unj-E1V"
   },
   "outputs": [],
   "source": [
    "gcs_input_path = 'gs://cloud-datalab-samples/cars.csv'\n",
    "gcs_output_path = 'gs://%(bucket_name)s/all_makes_%(_ts_nodash)s.csv'\n",
    "pipeline_name = 'gcs_to_gcs_transform_' + formatted_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "11R6SnbSOokH"
   },
   "outputs": [],
   "source": [
    "%%bq query --name all_makes\n",
    "SELECT Make FROM input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xqGHWFlLex_V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Composer pipeline successfully deployed! View dashboard for more details.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq pipeline --name $pipeline_name -e $environment -l $location\n",
    "input:\n",
    "  path: $gcs_input_path\n",
    "  schema:\n",
    "    - name: Year\n",
    "      type: INTEGER\n",
    "    - name: Make\n",
    "      type: STRING\n",
    "    - name: Model\n",
    "      type: STRING\n",
    "    - name: Description\n",
    "      type: STRING\n",
    "    - name: Price\n",
    "      type: FLOAT\n",
    "  csv:\n",
    "    skip: 1\n",
    "transformation: \n",
    "  query: all_makes\n",
    "output:\n",
    "  path: $gcs_output_path\n",
    "parameters:\n",
    "  - name: bucket_name\n",
    "    type: STRING\n",
    "    value: $bucket_name\n",
    "schedule:\n",
    "  start: $start\n",
    "  end:  $end\n",
    "  interval: '@once'\n",
    "  catchup: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16QvAy_WSyDn"
   },
   "source": [
    "# Load data from GCS into BigQuery\n",
    "*pipeline* can also be used to parameterize and schedule the loading of data from GCS to BQ, i.e the equivalent of the *%bq load* command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RVwTNF4jnGA7"
   },
   "outputs": [],
   "source": [
    "bq_load_results_table = '%(project)s.%(dataset_name)s.cars_load'\n",
    "pipeline_name = 'load_gcs_to_bq_' + formatted_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-qHQ4bfkXwEU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Composer pipeline successfully deployed! View dashboard for more details.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq pipeline --name $pipeline_name -e $environment -l $location\n",
    "load:\n",
    "  path: $gcs_input_path\n",
    "  schema:\n",
    "    - name: Year\n",
    "      type: INTEGER\n",
    "    - name: Make\n",
    "      type: STRING\n",
    "    - name: Model\n",
    "      type: STRING\n",
    "    - name: Description\n",
    "      type: STRING\n",
    "    - name: Price\n",
    "      type: FLOAT\n",
    "  csv:\n",
    "    skip: 1\n",
    "  table: $bq_load_results_table\n",
    "  mode: overwrite\n",
    "parameters:\n",
    "  - name: project\n",
    "    type: STRING\n",
    "    value: $project\n",
    "  - name: dataset_name\n",
    "    type: STRING\n",
    "    value: $dataset_name\n",
    "schedule:\n",
    "  start: $start\n",
    "  end: $end\n",
    "  interval: '@once'\n",
    "  catchup: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnuNrYDCXiXT"
   },
   "source": [
    "# Extract data from BigQuery into GCS\n",
    "Similar to load, *pipeline* can also be used to perform the equivalent of the *%bq extract* command. To illustrate, we extract the data in the table that was the result of the 'load' pipeline, and write it to a GCS file. \n",
    "\n",
    "Now, it's possible that if you \"Ran All Cells\" in this notebook, this pipeline gets deployed at the same time as the previous load-pipeline, in which case the source table isn't yet ready. Hence we set *retries* to 3, with a delay of 90 seconds and hope that the table eventually does get created and this pipeline is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nWgnLdX9YH7e"
   },
   "outputs": [],
   "source": [
    "gcs_extract_path = 'gs://%(bucket_name)s/cars_extract_%(_ts_nodash)s.csv'\n",
    "pipeline_name = 'extract_bq_to_gcs_' + formatted_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uu1YuNJIFpTM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Composer pipeline successfully deployed! View dashboard for more details.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq pipeline --name $pipeline_name -e $environment -l $location\n",
    "extract:\n",
    "  table: $bq_load_results_table\n",
    "  path: $gcs_extract_path\n",
    "  format: csv\n",
    "  csv:\n",
    "    delimiter: '#'\n",
    "parameters:\n",
    "  - name: bucket_name\n",
    "    type: STRING\n",
    "    value: $bucket_name\n",
    "  - name: project\n",
    "    type: STRING\n",
    "    value: $project\n",
    "  - name: dataset_name\n",
    "    type: STRING\n",
    "    value: $dataset_name\n",
    "schedule:\n",
    "  start: $start\n",
    "  interval: '@once'\n",
    "  catchup: True\n",
    "  retries: 3\n",
    "  retry_delay_seconds: 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjknsf6HIbJ_"
   },
   "source": [
    "# Output of successful pipeline runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5936,
     "status": "ok",
     "timestamp": 1517531275419,
     "user": {
      "displayName": "Rajiv Bharadwaja",
      "userId": "114213070630621217486"
     },
     "user_tz": 480
    },
    "id": "7N-r2h5bK9ml",
    "outputId": "d19b4d4e-87df-41e6-a1e6-0ae965541e34"
   },
   "outputs": [],
   "source": [
    "# You will see two files named all_makes_<timestamp> and cars_extract_<timestamp>\n",
    "# under the bucket:\n",
    "!gsutil ls gs://$bucket_name\n",
    "  \n",
    "# You will see three tables named cars_load, pr_events_<timestamp> and \n",
    "# <user>_pr_events_<timestamp> under the BigQuery dataset:\n",
    "!bq ls $dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyph2e8WhlCf"
   },
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "G7MApPqchoul"
   },
   "outputs": [],
   "source": [
    "# Delete the contents of the GCS bucket, the GCS bucket itself, and the BQ \n",
    "# dataset. Uncomment the lines below and execute.\n",
    "#!gsutil rm -r gs://$bucket_name\n",
    "#!bq rm -r -f $dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCJPb1RVQbjq"
   },
   "source": [
    "# Stop Billing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lutR46eiQb_6"
   },
   "outputs": [],
   "source": [
    "# If you chose the Airflow VM (in the Setup), this will delete the VM. Uncomment the \n",
    "# line below and execute.\n",
    "#!gcloud compute instances stop $vm_name --zone us-central1-b --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YU5SK7_lj7_S"
   },
   "outputs": [],
   "source": [
    "# This just verifies that cleanup actually worked. Run this after running the \n",
    "# 'Cleanup' cell\n",
    "\n",
    "#Should show two error messages like \"BucketNotFoundException: 404 gs://...\"\n",
    "!gsutil ls gs://$bucket_name\n",
    "!gsutil ls gs://$gcs_dag_bucket_name/dags\n",
    "  \n",
    "#Should show an error message like \"BigQuery error in ls operation: Not found ...\"\n",
    "!bq ls $dataset_name\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "BigQuery Pipeline.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
