{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDFs using Code in Cloud Storage\n",
    "\n",
    "This notebook shows you how to use Javascript UDFs (user-defined functions) in Google BigQuery that reference Javascript code stored in Google Cloud Storage. Storing your UDF support code in Cloud Storage allows you to reuse well tested code and share it across multiple notebooks.\n",
    "\n",
    "Before using this tutorial, you should go through the [UDFs in BigQuery](notebooks/datalab/tutorials/BigQuery/UDFs%20in%20BigQuery.ipynb) tutorial, which discusses how to use UDFs in notebooks without external code, and the [UDF Testing in the Notebook](notebooks/datalab/tutorials/BigQuery/UDF%20Testing%20in%20the%20Notebook.ipynb) tutorial, which shows you how to run and test your Javascript code in the notebook.\n",
    "\n",
    "You can read more about UDFs [here](https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "This notebook repeats a scenario presented in other notebooks: looking at anonymized logs that originated in Google AppEngine. \n",
    "\n",
    "With BigQuery, it is possible to store and reference your UDFs in Google Cloud Storage. Let's see how we can do this within Datalab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving the Code to GCS\n",
    "\n",
    "Recall from the previous notebook [UDF Testing in the Notebook](notebooks/datalab/tutorials/BigQuery/UDF%20Testing%20in%20the%20Notebook.ipynb) how we can add a Javascript UDF and test it. We can use the Javascript IPython kernel module to communicate the UDF code over to the Python environment after we test it, where we can then use it in other Python cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "/**\n",
    " * Our UDF function, which splits a set of URL query parameters into an array\n",
    " */\n",
    "function getParameters(path) {\n",
    "  var re = /[?&]([^=]*)=([^&]*)/g;\n",
    "  var result = [];\n",
    "  var match;\n",
    "  while ((match = re.exec(path)) != null) {\n",
    "    result.push(decodeURIComponent(match[2]));\n",
    "  }\n",
    "  return result;\n",
    "}\n",
    "\n",
    "// Now we want to test the UDF. We can try calling it using a sample line from our table.\n",
    "// Note that the variable `element` is available to us to create output in the notebook,\n",
    "// so our test emitter will use that variable to display the fields.\n",
    "var test_path = '/log/page?project=14&instance=81&user=16&page=master&path=63&version=0.1.1&release=alpha'\n",
    "var test_results = ['14', '81', '16', 'master', '63', '0.1.1', 'alpha']\n",
    "\n",
    "var params = getParameters(test_path);\n",
    "var code = 'JsUDFCode = None'\n",
    "if (JSON.stringify(params) == JSON.stringify(test_results)) {\n",
    "  element.append('OK')\n",
    "  code = 'JsUDFCode = \"\"\"' + getParameters.toString() + '\"\"\"';\n",
    "} else {\n",
    "  element.append('Failed')\n",
    "}\n",
    "\n",
    "IPython.notebook.kernel.execute(code);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we copied over the code from Javascript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "JsUDFCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the testing is done, let's create a file in Cloud Storage to hold the bulk of the code. We can do that in the notebook. The name of the bucket will be project dependent, so you will need to complete and execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.datalab import Context\n",
    "import google.datalab.storage as gs\n",
    "\n",
    "# Get a bucket in the current project\n",
    "project = Context.default().project_id\n",
    "sample_bucket_name = project + '-datalab-udf-samples'\n",
    "\n",
    "# Create the storage bucket and code library object\n",
    "sample_bucket = gs.Bucket(sample_bucket_name)\n",
    "sample_bucket.create()\n",
    "sample_object = sample_bucket.object('udf_library.js')\n",
    "sample_object.write_stream(JsUDFCode, 'application/javascript')\n",
    "\n",
    "# Print the URI of the library object to use in @import\n",
    "print sample_object.uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code is copied to Cloud Storage, we can refer to it in the UDF jsdoc comment header using `@import`. You can have more than one `@import` if necessary. Note that in the cell below, you need to change the `@import` to refer to your project. You can use the URI output from the cell above as the argument to `@import`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bq udf -n externalized_udf -l js\n",
    "// @returns ARRAY<STRING>\n",
    "// @param path STRING\n",
    "// @import gs://yelsayed-project1-datalab-udf-samples/udf_library.js\n",
    "return getParameters(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bq query -n log_query --udfs externalized_udf\n",
    "SELECT *, externalized_udf(path) as parameters FROM `cloud-datalab-samples.appenginelogs.sample_logs_20151027`\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bq execute -q log_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the query to see its expanded SQL, we can see all it has is a simple UDF that imports some code from the Cloud Storage location we stored it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "Since this is a tutorial, we should clean up the objects we created in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_object.delete()\n",
    "sample_bucket.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
