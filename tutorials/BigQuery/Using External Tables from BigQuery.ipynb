{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using External Tables from BigQuery\n",
    "\n",
    "Google BigQuery has the ability to query data directly from Google Cloud Storage (a feature called \"External Data Sources\"). This feature can be useful when querying small amounts of data that you may not want to load into a BigQuery table. It is not recommended for large queries, however, because BigQuery billing is based on the amount of data read to process a query. BigQuery can very efficiently query subsets of tables in its own store since these are stored in columnar format, so the unused columns are not read and don't add to the cost. But since data stored in Cloud Storage is typically in the form of a compressed CSV file, typically, the entire file must be read. Hence, while querying data in Cloud Storage can he helpful, it should be used judiciously. \n",
    "\n",
    "In this notebook we will show you how to download data from a source on the Internet, put it in Cloud Storage, and then query it directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data and Loading into GCS\n",
    "\n",
    "For this sample we want to use external data in a CSV, load it into Cloud Storage, and query it. We will use the Seattle bike station data from the [Pronto 2015 Data Challenge dataset](https://www.prontocycleshare.com/datachallenge).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.datalab import Context\n",
    "import google.datalab.bigquery as bq\n",
    "import google.datalab.storage as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "\n",
    "data_source = \"https://storage.googleapis.com/cloud-datalab-samples/udfsample/2015_station_data.csv\"\n",
    "\n",
    "f = urllib2.urlopen(data_source)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "print 'Read %d bytes' % len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a bucket in the current project\n",
    "project = Context.default().project_id\n",
    "sample_bucket_name = project + '-station_data'\n",
    "\n",
    "# Create and write to the GCS item\n",
    "sample_bucket = gs.Bucket(sample_bucket_name)\n",
    "sample_bucket.create()\n",
    "sample_object = sample_bucket.object('station_data.csv')\n",
    "sample_object.write_stream(data, 'text/plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an External Data Source Object\n",
    "\n",
    "Now we need to create a special `ExternalDataSource` object that refers to the data, which can, in turn, be used as a table in our BigQuery queries. We need to provide a schema for BigQuery to use the data. The CSV file has a header row that we want to skip; we will use a `CSVOptions` object to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options = bq.CSVOptions(skip_leading_rows=1) # Skip the header row\n",
    "\n",
    "schema = bq.Schema([\n",
    "  {'name': 'id', 'type': 'INTEGER'},         # row ID\n",
    "  {'name': 'name', 'type': 'STRING'},        # friendly name\n",
    "  {'name': 'terminal', 'type': 'STRING'},    # terminal ID\n",
    "  {'name': 'lat', 'type': 'FLOAT'},          # latitude\n",
    "  {'name': 'long', 'type': 'FLOAT'},         # longitude\n",
    "  {'name': 'dockcount', 'type': 'INTEGER'},  # bike capacity\n",
    "  {'name': 'online', 'type': 'STRING'}       # date station opened\n",
    "])\n",
    "\n",
    "drivedata = bq.ExternalDataSource(source=sample_object.uri, # The gs:// URL of the file \n",
    "                                  csv_options=options,\n",
    "                                  schema=schema,\n",
    "                                  max_bad_records=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Table\n",
    "\n",
    "Now let's verify that we can access the data. We will run a simple query to show the first five rows. Note that we specify the federated table by using a name in the query, and then pass the table in using a `data_sources` dictionary parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bq.Query('SELECT * FROM drivedata LIMIT 5', data_sources=['drivedata']).execute().result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_object.delete()\n",
    "sample_bucket.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
