{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training requires a tarball python package that includes your training program based on TensorFlow. In the Iris sample, we demonstrates how to implement model training using tf.learn framework. Now we will demo how to use vannila TensorFlow only to implement a distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Package\n",
    "\n",
    "You can use existing tarball package (locally or in GCS), or use your own tarball package. You can define a python module use \"%%mlalpha module\". In the following two cells, we will define two python modules: \"census\" and \"task\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%mlalpha module --name census\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "import math\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\n",
    "  thread_count = multiprocessing.cpu_count()\n",
    "  # The minimum number of instances in a queue from which examples are drawn\n",
    "  # randomly. The larger this number, the more randomness at the expense of\n",
    "  # higher memory requirements.\n",
    "  MIN_AFTER_DEQUEUE = 100\n",
    "\n",
    "  # When batching data, the queue's capacity will be larger than the batch_size\n",
    "  # by some factor. The recommended formula is (num_threads + a small safety\n",
    "  # margin). For now, we use a single thread for reading, so this can be small.\n",
    "  QUEUE_SIZE_MULTIPLIER = thread_count + 3\n",
    "\n",
    "  # Convert num_epochs == 0 -> num_epochs is None, if necessary\n",
    "  num_epochs = num_epochs or None\n",
    "\n",
    "  # input_files could be a path to one file or a file pattern.\n",
    "  input_file_names = tf.train.match_filenames_once(input_files)\n",
    "  \n",
    "  # Build a queue of the filenames to be read.\n",
    "  filename_queue = tf.train.string_input_producer(input_file_names, num_epochs,\n",
    "                                                  shuffle)\n",
    "\n",
    "  options = tf.python_io.TFRecordOptions(\n",
    "      compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "  example_id, encoded_example = tf.TFRecordReader(options=options).read_up_to(\n",
    "      filename_queue, batch_size)\n",
    "\n",
    "  if shuffle:\n",
    "    capacity = MIN_AFTER_DEQUEUE + QUEUE_SIZE_MULTIPLIER * batch_size\n",
    "    return tf.train.shuffle_batch([example_id, encoded_example], batch_size,\n",
    "                                  capacity, MIN_AFTER_DEQUEUE,\n",
    "                                  enqueue_many=True, num_threads=thread_count)\n",
    "  else:\n",
    "    capacity = QUEUE_SIZE_MULTIPLIER * batch_size\n",
    "    return tf.train.batch([example_id, encoded_example],\n",
    "                          batch_size, capacity=capacity,\n",
    "                          enqueue_many=True, num_threads=thread_count)\n",
    "\n",
    "def create_inputs(metadata, input_data=None):\n",
    "  with tf.name_scope('inputs'):\n",
    "    if input_data is None:\n",
    "      input_data = tf.placeholder(tf.string, name='input', shape=(None,))\n",
    "\n",
    "    example_schema = {}\n",
    "    example_schema['inputs'] = tf.FixedLenFeature(\n",
    "        shape=[metadata.features['inputs']['size']], \n",
    "        dtype=tf.float32)\n",
    "    example_schema['key'] = tf.FixedLenFeature(\n",
    "        shape=[metadata.features['key']['size']], \n",
    "        dtype=tf.string)\n",
    "    example_schema['target'] = tf.FixedLenFeature(\n",
    "        shape=[metadata.features['target']['size']], \n",
    "        dtype=tf.float32, default_value=0.0)\n",
    "    parsed_examples = tf.parse_example(input_data, example_schema)\n",
    "    \n",
    "    parsed_features = {}\n",
    "    parsed_features['inputs'] = parsed_examples['inputs']\n",
    "    parsed_features['key'] = parsed_examples['key']\n",
    "    parsed_features['target'] = parsed_examples['target']\n",
    "\n",
    "    return (input_data, parsed_features['inputs'], tf.squeeze(parsed_features['target']),\n",
    "            tf.identity(parsed_features['key']))\n",
    "\n",
    "def _create_layer(inputs, input_size, output_size):\n",
    "  with tf.name_scope('layer'):\n",
    "    initial_weights = tf.truncated_normal([input_size, output_size],\n",
    "                                          stddev = 1.0 / math.sqrt(input_size))\n",
    "    weights = tf.Variable(initial_weights, name = 'weights')\n",
    "\n",
    "    initial_biases = tf.zeros([ output_size ])\n",
    "    biases = tf.Variable(initial_biases, name = 'biases')\n",
    "\n",
    "    xw = tf.matmul(inputs, weights)\n",
    "\n",
    "    return tf.nn.bias_add(xw, biases)\n",
    "\n",
    "def inference(inputs, metadata, hyperparams):\n",
    "  input_size = metadata.features['inputs']['size']\n",
    "  output_size = metadata.features['target']['size']\n",
    "\n",
    "  hidden_layer1 = tf.nn.relu(_create_layer(inputs, input_size,\n",
    "                                           hyperparams['hidden_layer1_size']))\n",
    "  hidden_layer2 = tf.nn.relu(_create_layer(hidden_layer1,\n",
    "                                           hyperparams['hidden_layer1_size'],\n",
    "                                           hyperparams['hidden_layer2_size']))\n",
    "  hidden_layer3 = tf.nn.relu(_create_layer(hidden_layer2,\n",
    "                                           hyperparams['hidden_layer2_size'],\n",
    "                                           hyperparams['hidden_layer3_size']))\n",
    "  output = _create_layer(hidden_layer3, hyperparams['hidden_layer3_size'],\n",
    "                         output_size)\n",
    "  return output\n",
    "\n",
    "\n",
    "def loss(output, targets):\n",
    "  \"\"\"Calculates the loss from the output and the labels.\n",
    "  Args:\n",
    "    output: output layer tensor, float - [batch_size].\n",
    "    targets: Target value tensor, float - [batch_size].\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  loss = tf.reduce_mean(tf.abs(output - targets), name = 'loss')\n",
    "  return loss\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "  with tf.name_scope('train'):\n",
    "    tf.scalar_summary(loss.op.name, loss)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step)\n",
    "    return train_op, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha module --name task --main\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import census\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.metrics.python.ops import metric_ops\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "import google.cloud.ml.util as cloudml_util\n",
    "\n",
    "EXPORT_SUBDIRECTORY = 'model'\n",
    "HYPERPARAMS = {\n",
    "  'batch_size': 32,\n",
    "  'learning_rate': 0.001,\n",
    "}\n",
    "EVAL_SET_SIZE = 2767\n",
    "EVAL_INTERVAL_SECS = 3\n",
    "\n",
    "\n",
    "def main():\n",
    "  config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "  cluster = config.get('cluster', None)\n",
    "  task = config.get('task', None)\n",
    "  job = config.get('job', None)\n",
    "  trial_id = task.get('trial', '')\n",
    "  tf.logging.info(\"start trial %s.\", trial_id)\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--train_data_paths\", type=str)\n",
    "  parser.add_argument(\"--eval_data_paths\", type=str)\n",
    "  parser.add_argument(\"--metadata_path\", type=str)\n",
    "  parser.add_argument(\"--output_path\", type=str)\n",
    "  parser.add_argument(\"--max_steps\", type=int, default=2000)\n",
    "  parser.add_argument(\"--hidden1\", type=int, default=300)\n",
    "  parser.add_argument(\"--hidden2\", type=int, default=200)\n",
    "  parser.add_argument(\"--hidden3\", type=int, default=100)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  dispatch(args, cluster, task, job, trial_id)\n",
    "\n",
    "\n",
    "def start_server(cluster, task):\n",
    "  # Create and start a server.\n",
    "  return tf.train.Server(cluster,\n",
    "                         protocol=\"grpc\",\n",
    "                         job_name=task['type'],\n",
    "                         task_index=task['index'])\n",
    "\n",
    "\n",
    "def dispatch(args, cluster, task, job, trial_id):\n",
    "  if not cluster:\n",
    "    # Run locally.\n",
    "    run_training(args, target=\"\", is_chief=True, device_fn=\"\", trial_id=trial_id)\n",
    "    return\n",
    "\n",
    "  if task['type'] == \"ps\":\n",
    "    server = start_server(cluster, task)\n",
    "    server.join()\n",
    "  elif task['type'] == \"worker\":\n",
    "    server = start_server(cluster, task)\n",
    "    is_chief = False\n",
    "    device_fn = tf.train.replica_device_setter(\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:worker/task:%d\" % task['index'],\n",
    "        cluster=cluster)\n",
    "    run_training(args, server.target, is_chief, device_fn, trial_id)\n",
    "  elif task['type'] == \"master\":\n",
    "    server = start_server(cluster, task)\n",
    "    is_chief = (task['index'] == 0)\n",
    "    device_fn = tf.train.replica_device_setter(\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:master/task:%d\" % task['index'],\n",
    "        cluster=cluster)\n",
    "    run_training(args, server.target, is_chief, device_fn, trial_id)\n",
    "  else:\n",
    "    raise ValueError(\"invalid job_type %s\" % task['type'])\n",
    "\n",
    "\n",
    "def run_training(args, target, is_chief, device_fn, trial_id):\n",
    "  \"\"\"Train Census for a number of steps.\"\"\"\n",
    "  output_path = os.path.join(args.output_path, trial_id)\n",
    "  # Get the sets of examples and targets for training, validation, and\n",
    "  # test on Census.\n",
    "  training_data = args.train_data_paths\n",
    "  if is_chief:\n",
    "    # A generator over accuracies. Each call to next(accuracies) forces an\n",
    "    # evaluation of the model.\n",
    "    accuracies = evaluate(args, trial_id)\n",
    "\n",
    "  # Tell TensorFlow that the model will be built into the default Graph.\n",
    "  with tf.Graph().as_default() as graph:\n",
    "    # Assigns ops to the local worker by default.\n",
    "    with tf.device(device_fn):\n",
    "\n",
    "      metadata = features.FeatureMetadata.get_metadata(args.metadata_path)\n",
    "\n",
    "      _, train_examples = census.read_examples(\n",
    "          training_data, HYPERPARAMS['batch_size'], shuffle=False)\n",
    "\n",
    "      # Generate placeholders for the examples.\n",
    "      placeholder, inputs, targets, _ = (\n",
    "          census.create_inputs(metadata, input_data=train_examples))\n",
    "\n",
    "      # Build a Graph that computes predictions from the inference model.\n",
    "      layer_sizes = {\n",
    "        'hidden_layer1_size': args.hidden1,\n",
    "        'hidden_layer2_size': args.hidden2,\n",
    "        'hidden_layer3_size': args.hidden3,\n",
    "      }\n",
    "      output = census.inference(inputs, metadata, layer_sizes)\n",
    "\n",
    "      # Add to the Graph the Ops for loss calculation.\n",
    "      loss = census.loss(output, targets)\n",
    "\n",
    "      # Add to the Graph the Ops that calculate and apply gradients.\n",
    "      train_op, global_step = census.training(loss,\n",
    "                                              HYPERPARAMS['learning_rate'])\n",
    "\n",
    "      # Build the summary operation based on the TF collection of Summaries.\n",
    "      summary_op = tf.merge_all_summaries()\n",
    "\n",
    "      # Add the variable initializer Op.\n",
    "      init_op = tf.initialize_all_variables()\n",
    "\n",
    "      # Create a saver for writing training checkpoints.\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "      summary_writer = tf.train.SummaryWriter(os.path.join(\n",
    "          output_path, 'summaries'), graph)\n",
    "\n",
    "      # Create a \"supervisor\", which oversees the training process.\n",
    "      sv = tf.train.Supervisor(is_chief=is_chief,\n",
    "                               logdir=os.path.join(output_path, 'logdir'),\n",
    "                               init_op=init_op,\n",
    "                               saver=saver,\n",
    "                               summary_op=None,\n",
    "                               global_step=global_step,\n",
    "                               save_model_secs=60)\n",
    "\n",
    "      # The supervisor takes care of session initialization, restoring from\n",
    "      # a checkpoint, and closing when done or an error occurs.\n",
    "      tf.logging.info(\"Starting the loop.\")\n",
    "      with sv.managed_session(target) as sess:\n",
    "        start_time = time.time()\n",
    "        last_save = start_time\n",
    "\n",
    "        # Loop until the supervisor shuts down or max_steps have completed.\n",
    "        step = 0\n",
    "        while not sv.should_stop() and step < args.max_steps:\n",
    "          start_time = time.time()\n",
    "\n",
    "          # Run one step of the model.  The return values are the activations\n",
    "          # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "          # inspect the values of your Ops or variables, you may include them\n",
    "          # in the list passed to sess.run() and the value tensors will be\n",
    "          # returned in the tuple from the call.\n",
    "          _, step, loss_value = sess.run([train_op, global_step, loss])\n",
    "\n",
    "          duration = time.time() - start_time\n",
    "          if is_chief and time.time() - last_save > EVAL_INTERVAL_SECS:\n",
    "            saver.save(sess, sv.save_path, global_step)\n",
    "            accuracy = next(accuracies)\n",
    "            last_save = time.time()\n",
    "            tf.logging.info(\"Eval, step %d: error = %0.3f\", step, accuracy)\n",
    "\n",
    "          # Write the summaries and log an overview fairly often.\n",
    "          if step % 100 == 0 and is_chief:\n",
    "            tf.logging.info(\"Step %d: loss = %.2f (%.3f sec)\" % (step, loss_value, duration))\n",
    "\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "\n",
    "        if is_chief:\n",
    "          # Force a save at the end of our loop.\n",
    "          sv.saver.save(sess, sv.save_path, global_step=global_step,\n",
    "                        write_meta_graph=False)\n",
    "          accuracy_value = next(accuracies)\n",
    "          tf.logging.info(\"Final error after %d steps = %0.3f\", step, accuracy_value)\n",
    "\n",
    "          # Save the model for inference\n",
    "          export_model(args, sess, sv.saver, trial_id)\n",
    "\n",
    "      # Ask for all the services to stop.\n",
    "      sv.stop()\n",
    "      tf.logging.info(\"Done training.\")\n",
    "\n",
    "\n",
    "def export_model(args, sess, training_saver, trial_id):\n",
    "  output_path = os.path.join(args.output_path, trial_id)\n",
    "  with tf.Graph().as_default() as inference_graph:\n",
    "    metadata = features.FeatureMetadata.get_metadata(args.metadata_path)\n",
    "    placeholder, inputs, _, keys = census.create_inputs(metadata)\n",
    "    layer_sizes = {\n",
    "      'hidden_layer1_size': args.hidden1,\n",
    "      'hidden_layer2_size': args.hidden2,\n",
    "      'hidden_layer3_size': args.hidden3,\n",
    "    }\n",
    "    output = census.inference(inputs, metadata, layer_sizes)\n",
    "\n",
    "    inference_saver = tf.train.Saver()\n",
    "\n",
    "    # Mark the inputs and the outputs\n",
    "    tf.add_to_collection(\"inputs\",\n",
    "                         json.dumps({\"examples\": placeholder.name}))\n",
    "    tf.add_to_collection('outputs',\n",
    "                         json.dumps({'key': tf.squeeze(keys).name,\n",
    "                                     'predicted': tf.squeeze(output).name}))\n",
    "\n",
    "    model_dir = os.path.join(output_path, EXPORT_SUBDIRECTORY)\n",
    "\n",
    "    # Save a copy of the metadata file used for this model with the exported\n",
    "    # model, so that online and batch prediction can use it.\n",
    "    subprocess.check_call(['gsutil', 'cp', args.metadata_path,\n",
    "                           os.path.join(model_dir, \"metadata.yaml\")])\n",
    "    \n",
    "    inference_saver.export_meta_graph(\n",
    "        filename=os.path.join(model_dir, \"export.meta\"))\n",
    "\n",
    "    # Save the variables. Don't write the MetaGraphDef, because that is\n",
    "    # actually the training graph.\n",
    "    training_saver.save(sess,\n",
    "                        os.path.join(model_dir, \"export\"),\n",
    "                        write_meta_graph=False)\n",
    "\n",
    "\n",
    "def evaluate(args, trial_id):\n",
    "  \"\"\"Run one round of evaluation, yielding accuracy.\"\"\"\n",
    "  output_path = os.path.join(args.output_path, trial_id)\n",
    "  eval_data = args.eval_data_paths\n",
    "\n",
    "  with tf.Graph().as_default() as g:\n",
    "    metadata = features.FeatureMetadata.get_metadata(args.metadata_path)\n",
    "\n",
    "    _, examples = census.read_examples(\n",
    "        eval_data, HYPERPARAMS['batch_size'],\n",
    "        shuffle=False)\n",
    "\n",
    "    # Generate placeholders for the examples.\n",
    "    placeholder, inputs, targets, _ = (\n",
    "        census.create_inputs(metadata, input_data=examples))\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    layer_sizes = {\n",
    "      'hidden_layer1_size': args.hidden1,\n",
    "      'hidden_layer2_size': args.hidden2,\n",
    "      'hidden_layer3_size': args.hidden3,\n",
    "    }\n",
    "    output = census.inference(inputs, metadata, layer_sizes)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss = census.loss(output, targets)\n",
    "\n",
    "    # Add the Op to compute accuracy.\n",
    "    error, eval_op = metric_ops.streaming_mean_relative_error(\n",
    "        output, targets, tf.ones(HYPERPARAMS['batch_size']))\n",
    "\n",
    "    # The global step is useful for summaries.\n",
    "    with tf.name_scope('train'):\n",
    "      global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    summary = tf.scalar_summary(\"error\", error)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "  num_eval_batches = float(EVAL_SET_SIZE) // HYPERPARAMS['batch_size']\n",
    "  summary_writer = tf.train.SummaryWriter(os.path.join(\n",
    "      output_path, 'eval'))\n",
    "\n",
    "  sv = tf.train.Supervisor(graph=g,\n",
    "                           logdir=os.path.join(output_path, 'eval'),\n",
    "                           summary_op=summary,\n",
    "                           summary_writer=summary_writer,\n",
    "                           global_step=None,\n",
    "                           saver=saver)\n",
    "\n",
    "  step = 0\n",
    "  while step < args.max_steps:\n",
    "    last_checkpoint = tf.train.latest_checkpoint(os.path.join(\n",
    "        output_path, 'logdir'))\n",
    "    with sv.managed_session(master=\"\",\n",
    "                            start_standard_services=False) as session:\n",
    "      sv.start_queue_runners(session)\n",
    "      sv.saver.restore(session, last_checkpoint)\n",
    "      accuracy = tf_evaluation(session,\n",
    "                               max_num_evals=num_eval_batches,\n",
    "                               eval_op=eval_op,\n",
    "                               final_op=error,\n",
    "                               summary_op=summary,\n",
    "                               summary_writer=summary_writer,\n",
    "                               global_step=global_step)\n",
    "\n",
    "      step = tf.train.global_step(session, global_step)\n",
    "      yield accuracy\n",
    "\n",
    "\n",
    "def tf_evaluation(sess,\n",
    "                  max_num_evals=1000,\n",
    "                  eval_op=None,\n",
    "                  final_op=None,\n",
    "                  summary_op=None,\n",
    "                  summary_writer=None,\n",
    "                  global_step=None):\n",
    "  if eval_op is not None:\n",
    "    try:\n",
    "      for i in range(int(max_num_evals)):\n",
    "        (_, final_op_value) = sess.run((eval_op, final_op))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      # We've hit the end of our epoch.  Unfortunately, if we hit this\n",
    "      # tensorflow has already logged a warning to stderr, so we try to avoid\n",
    "      # hitting it in this sample.\n",
    "      pass\n",
    "\n",
    "  if summary_op is not None:\n",
    "    if global_step is None:\n",
    "      raise ValueError(\"must specify global step\")\n",
    "\n",
    "    global_step = tf.train.global_step(sess, global_step)\n",
    "    summary = sess.run(summary_op)\n",
    "    hptuning_summary = tf.Summary(value=[\n",
    "      tf.Summary.Value(tag='training/hptuning/metric', simple_value=float(final_op_value))\n",
    "    ])\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "    summary_writer.add_summary(hptuning_summary, global_step)\n",
    "    summary_writer.flush()\n",
    "\n",
    "  return final_op_value\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"%mlalpha train\" to generate the training cell template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the required fields and run. <br>\n",
    "Datalab will simulate the CloudML service by creating master, worker, and ps processes (in cloud they are different VMs) to perform a distributed training, although all these processes run in the local container VM.<br>\n",
    "You can set worker_count and parameter_server_count to 1 to enable dedicated worker role and ps role. In example below, we only enable master.<br>\n",
    "The output of the training will be links to the processes output logs, and also refreshed every 3 seconds to show last few lines of the logs. You can use the local run to quickly validate your training program and parameters before submitting it to cloud to do large scale training.<br>\n",
    "If for any reasons the training is stuck, just click \"Reset Session\" to reset the kernel. All training processes will be cleaned up.<br>\n",
    "\n",
    "There are two ways you could specify a trainer program: you can specify \"package_uris\" and \"python_module\" in the input cell for existing tarball package. Or, if these are absent, it will look for all \"%%mlalpha module\" cells and create a temp tarball package to run. <br>\n",
    "\n",
    "Since we already defined our training modules, let's run the training program without explicitly specifying package. Datalab will create a temp package and will run the entrypoint module specified by \"--main\" flag in a \"%%mlalpha module\" cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: INFO:tensorflow:Step 1000: loss = 46797.48 (0.003 sec)<br/>master: INFO:tensorflow:Step 1100: loss = 44443.68 (0.003 sec)<br/>master: INFO:tensorflow:Step 1200: loss = 39139.96 (0.004 sec)<br/>master: INFO:tensorflow:Step 1300: loss = 59334.47 (0.002 sec)<br/>master: INFO:tensorflow:Step 1400: loss = 34802.27 (0.002 sec)<br/>master: INFO:tensorflow:Step 1500: loss = 46093.71 (0.004 sec)<br/>master: INFO:tensorflow:Step 1600: loss = 57432.68 (0.002 sec)<br/>master: INFO:tensorflow:Step 1700: loss = 53936.29 (0.003 sec)<br/>master: <br/>master: INFO:tensorflow:Step 1800: loss = 50924.97 (0.002 sec)<br/>master: INFO:tensorflow:Step 1900: loss = 46397.09 (0.003 sec)<br/>master: INFO:tensorflow:Eval, step 1985: error = 49197.320<br/>master: INFO:tensorflow:Step 2000: loss = 33515.07 (0.003 sec)<br/>master: INFO:tensorflow:Final error after 2000 steps = 50327.906<br/>master: Copying file:///content/datalab/tmp/ml/census/preprocessed/metadata.yaml...<br/>master: / [0 files][    0.0 B/ 14.3 KiB]                                                <br/>master: / [1 files][ 14.3 KiB/ 14.3 KiB]                                                <br/>master: Operation completed over 1 objects/14.3 KiB.                                     <br/>master: INFO:tensorflow:Done training.<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mlalpha train\n",
    "args:\n",
    "  train_data_paths: /content/datalab/tmp/ml/census/preprocessed/features_train*\n",
    "  eval_data_paths: /content/datalab/tmp/ml/census/preprocessed/features_eval*\n",
    "  metadata_path: /content/datalab/tmp/ml/census/preprocessed/metadata.yaml\n",
    "  output_path: /content/datalab/tmp/ml/census/model\n",
    "  hidden1: 200\n",
    "  hidden2: 100\n",
    "  hidden3: 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the training output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval  logdir  model  summaries\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/datalab/tmp/ml/census/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start TensorBoard to view training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 18875. Click <a href=\"/_proxy/44407/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir /content/datalab/tmp/ml/census/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the tensorboard serverwhen you are done with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 18875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train another model with larger hidden layer sizes.\n",
    "Instead of running the modules defined by \"%%mlalpha modules\" directly, we will package the modules first. <br>\n",
    "\"%%mlalpha package\" gathers all modules defined by \"%%mlalpha modules\" and create a tarball package that CloudML training service can consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package created at /content/datalab/tmp/ml/census/trainer-0.1.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "%%mlalpha package --out /content/datalab/tmp/ml/census/ --name trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the package explicitly by package_uris. This time we set 'parameter_server_count' and 'worker_count' to 1. Note that the training output will contain three log links: master, worker, and ps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/worker\" target=\"_blank\">worker log</a>&nbsp;&nbsp;<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;<a href=\"/_nocachecontent/ps\" target=\"_blank\">ps log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "ps: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:45181}<br/>ps: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:56835<br/>ps: <br/>master: INFO:tensorflow:train/global_step/sec: 0<br/>master: INFO:tensorflow:Step 400: loss = 40209.03 (0.004 sec)<br/>master: INFO:tensorflow:Step 700: loss = 59091.49 (0.005 sec)<br/>master: INFO:tensorflow:Step 800: loss = 51570.96 (0.007 sec)<br/>master: <br/>master: INFO:tensorflow:Eval, step 1126: error = 50016.734<br/>master: INFO:tensorflow:Step 1700: loss = 46015.92 (0.005 sec)<br/>master: <br/>master: INFO:tensorflow:Final error after 2002 steps = 50978.082<br/>master: Copying file:///content/datalab/tmp/ml/census/preprocessed/metadata.yaml...<br/>master: / [0 files][    0.0 B/ 14.3 KiB]                                                <br/>master: / [1 files][ 14.3 KiB/ 14.3 KiB]                                                <br/>master: Operation completed over 1 objects/14.3 KiB.                                     <br/>master: INFO:tensorflow:Done training.<br/>master: <br/>worker: INFO:tensorflow:Done training.<br/>worker: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mlalpha train\n",
    "package_uris: /content/datalab/tmp/ml/census/trainer-0.1.tar.gz\n",
    "python_module: trainer.task\n",
    "parameter_server_count: 1\n",
    "worker_count: 1\n",
    "args:\n",
    "  train_data_paths: /content/datalab/tmp/ml/census/preprocessed/features_train*\n",
    "  eval_data_paths: /content/datalab/tmp/ml/census/preprocessed/features_eval*\n",
    "  metadata_path: /content/datalab/tmp/ml/census/preprocessed/metadata.yaml\n",
    "  output_path: /content/datalab/tmp/ml/census/largermodel\n",
    "  hidden1: 400\n",
    "  hidden2: 200\n",
    "  hidden3: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Training\n",
    "\n",
    "Cloud training is similar but with \"--cloud\" flag, and use all GCS paths instead of local paths. <br>\n",
    "You also need to make sure you have a project whitelisted for CloudML, and use \"%projects set project-id\" to set it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = 'gs://' + datalab_project_id() + '-sampledata'\n",
    "package_path = os.path.join(bucket, 'census', 'model', 'trainer-0.1.tar.gz')\n",
    "train_data_path = os.path.join(bucket, 'census', 'preprocessed', 'features_train*')\n",
    "eval_data_path = os.path.join(bucket, 'census', 'preprocessed', 'features_eval*')\n",
    "metadata_path = os.path.join(bucket, 'census', 'preprocessed', 'metadata.yaml')\n",
    "output_path = os.path.join(bucket, 'census', 'trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the model to Cloud Storage so training service can get it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/datalab/tmp/ml/census/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  5.5 KiB/  5.5 KiB]                                                \n",
      "Operation completed over 1 objects/5.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/datalab/tmp/ml/census/trainer-0.1.tar.gz $package_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training using the Cloud DataFlow output from the \"2. Preprocessing\" notebook. We choose a set of hidden layer sizes, and later we will show how to sweep hyperparameter values using CloudML service using hyperparameter tuning feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160929_154930\" was submitted successfully.<br/>Run \"%mlalpha jobs --name trainer_task_160929_154930\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-ml-test-automated&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160929_154930\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: $package_path\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-east1\n",
    "args:\n",
    "  train_data_paths: $train_data_path\n",
    "  eval_data_paths: $eval_data_path\n",
    "  metadata_path: $metadata_path\n",
    "  output_path: $output_path\n",
    "  hidden1: 200\n",
    "  hidden2: 100\n",
    "  hidden3: 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the job status as described in the output. You can also run \"%ml jobs --filter state!=SUCCEEDED\" to see all active ML jobs in that project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>createTime: '2016-09-29T15:49:31Z'\n",
       "jobId: trainer_task_160929_154930\n",
       "startTime: '2016-09-29T15:52:05Z'\n",
       "state: RUNNING\n",
       "trainingInput:\n",
       "  args: [--hidden3, '50', --hidden2, '100', --hidden1, '200', --output_path, 'gs://cloud-ml-test-automated-sampledata/census/trained',\n",
       "    --train_data_paths, 'gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train*',\n",
       "    --metadata_path, 'gs://cloud-ml-test-automated-sampledata/census/preprocessed/metadata.yaml',\n",
       "    --eval_data_paths, 'gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval*']\n",
       "  packageUris: ['gs://cloud-ml-test-automated-sampledata/census/model/trainer-0.1.tar.gz']\n",
       "  pythonModule: trainer.task\n",
       "  region: us-east1\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha jobs --name trainer_task_160929_154930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-test-automated-sampledata/census/trained/\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/trained/eval/\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/trained/logdir/\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/trained/summaries/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
