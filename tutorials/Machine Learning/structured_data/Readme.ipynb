{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mltoolbox package\n",
    "\n",
    "The mltoolbox package is a collection of packages designed for quickly building, evaluating, and deploying Tensorflow models. Generally, each package provides tools for the main steps in developing a model: preprocessing, training, evaluating, and prediction. In addition to this, each package is structured so that these steps can be performed locally or using cloud services on Google Cloud Platform, without code changes. \n",
    "\n",
    "This notebook is a quick introduction two mltoolbox packages: regression and classification for structured data, collectively called the 'structured data' package.\n",
    "\n",
    "<br>\n",
    "# The structured data package\n",
    "\n",
    "The structured data package (`mltoolbox.classification` and `mltoolbox.regression`) is designed for solving classification or regression problems where the data has a columnar structure. That is, the input data contains a set of features where each feature is a numerical value or a categorical value. \n",
    "\n",
    "As an example, consider the problem of predicting a car's value given its make, model, year and millage:\n",
    "\n",
    "\n",
    "| example_id \t| value \t| make  \t| model            \t| year \t| millage \t|\n",
    "|------------\t|-------\t|-------\t|------------------\t|------\t|---------\t|\n",
    "| 1          \t| 11000 \t| 'Mazda' \t| 'CZ-5'           \t| 2013 \t| 70000   \t|\n",
    "| 2          \t| 45000 \t| 'BMW'   \t| '6 Series'       \t| 2015 \t| 28000   \t|\n",
    "| 3          \t| 20000 \t| 'Ford'  \t| 'F150 Super Cab' \t| 2014 \t| 50000   \t|\n",
    "\n",
    "This table has five different column types:\n",
    "1. key column (example_id)\n",
    "1. target column (value)\n",
    "1. categorical string column (make, model)\n",
    "1. categorical number column (year)\n",
    "1. numerical column (millage)\n",
    "\n",
    "The structured data package can work with problems with these types of columns. The structured data package does not support image data; meaning, a column of your data cannot be an image file or file path to an image. However, the mltoolbox package does support image classification problems, see `mltoolbox.image`. Also, the structured data package does not support text columns. This means sentiment analysis problems are not supported.\n",
    "\n",
    "A standard workflow when using the structured data package includes running these four functions:\n",
    "\n",
    "1. `analyze`: Computes statistics over the training set used by the trainer. Unlike other packages in the mltoolbox package, the structured data package does not have a preprocess step. The input data does not need any transformation into some other format.\n",
    "1. `train`: Starts a Tensorflow training job\n",
    "1. `batch_predict`: Runs prediction where the data is stored in files. When these files contain the target column, this step is called evaluation.\n",
    "1. `predict`: Runs prediction from an in-memory object. The cloud service version of this step makes a prediction call to a deployed model.\n",
    "\n",
    "Each of these functions can run its task locally or using GCP services, based on the inclusion or absence of a `cloud` parameter. Also, each of these function have an asynchronous version (`analyze_async`, `train_async`, etc) that returns a job object you can use to query the status of the task. This could be useful, for example, when submitting multiple ML Engine training jobs within one notebook at the same time.\n",
    "\n",
    "<br>\n",
    "# Structure of the notebook samples\n",
    "\n",
    "The notebook samples in this folder first demonstrate running analysis, training, batch prediction, and online prediction 'locally' without using GCP services. This first 'local end to end' notebook writes the output of each state to the local file system. The later notebooks demonstrates one of the four steps using cloud services. \n",
    "\n",
    "To speed things up and reduce cost, the cloud service notebooks extracts the results of the previous step from the local notebook. For example, the notebooks that perform cloud prediction require a trained model to exist in GCS. Instead of running training using the ML Engine, the cloud  prediction notebooks copy the trained model that the first local notebook make from the local file system to a location on GCS. This means you only pay for the cloud service the individual notebook demonstrates! Of course, the cost of running the cloud service notebooks is small as the datasets are very small.\n",
    "\n",
    "<br>\n",
    "# Key column\n",
    "\n",
    "In this package, it is required that the dataset contains a key column. This key column is actually not used in any step of this package. In fact the key column does not even have to contain unique values. However, a key column is required because it is extremely useful in batch prediction. Batch prediction in general does not produce predictions in the same order as the input. This means without a key value, it would be impossible to align the predicted value with the input! By building a key into the model, you can join prediction with the input data source or other external sources that use the same key.\n",
    "\n",
    "<br>\n",
    "# Where can I get help?\n",
    "\n",
    "Please post a question to Stack Overflow with the tag 'google-cloud-datalab'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
