{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "===\n",
    "\n",
    "<a href=\"#about\">About this notebook</a> <br />\n",
    "<a href=\"#setup\">Setting things up</a>\n",
    "\n",
    "Local Experience\n",
    "1. <a href=\"#local_preprocessing\">Local preprocessing starting from csv files</a>\n",
    "1. <a href=\"#local_training\">Local training</a>\n",
    "1. <a href=\"#local_prediction\">Local prediction</a>\n",
    "1. <a href=\"#local_batch_prediction\">Local batch prediction</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"about\"></a>\n",
    "About this notebook\n",
    "======\n",
    "\n",
    "This notebook uses the datalab structured data package for building and running a Tensorflow regression problem locally. This notebook uses data from the US Census Bureau 2014 American Community Survey for the state of South Dakota.\n",
    "\n",
    "In the notebooks that follow, an example of running preprocessing, training, and prediction using the Google Cloud Machine Learning Engine services are given. Note that running the cloud versions of preprocessing, training, and prediction take longer than the local versions. The performance advantage of using the cloud applies to very large data sets, and you don't see it with this sample because the data is small and run time is dominated by setup overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "Setting things up\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datalab_structured_data as sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the versions of structured_data and TF we have. Make sure TF is 1.0.0, and SD is 0.0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf 1.0.0\n",
      "sd 0.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "import datalab.ml as ml\n",
    "\n",
    "print('tf ' + str(tf.__version__))\n",
    "print('sd ' + str(sd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will write files during preprocessing, training, and prediction. Please give a root folder you wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOCAL_ROOT = './census_regression_workspace'\n",
    "if not file_io.file_exists(LOCAL_ROOT):\n",
    "  file_io.recursive_create_dir(LOCAL_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of the data is from the <a href=\"http://www2.census.gov/programs-surveys/acs/data/pums/2014/1-Year/csv_psd.zip\">US Census</a>, but we already have a copy on an public bucket on GCS. The raw data has many columns, and some of then are not useful. The next few cells downloads the data, keeps the interesting columns, and splits the data into a training, eval, and prediction set. Click <a href=\"http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict14.txt\">here</a> for a description of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying gs://cloud-ml-data/census/ss14psd.csv...\n",
      "/ [1 files][  7.8 MiB/  7.8 MiB]                                                \n",
      "Operation completed over 1 objects/7.8 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-ml-data/census/ss14psd.csv {LOCAL_ROOT}\n",
    "csv_columns = ('RT', 'SERIALNO', 'SPORDER', 'PUMA', 'ST', 'ADJINC', 'PWGTP',\n",
    "                 'AGEP', 'CIT', 'CITWP', 'COW', 'DDRS', 'DEAR', 'DEYE', 'DOUT',\n",
    "                 'DPHY', 'DRAT', 'DRATX', 'DREM', 'ENG', 'FER', 'GCL', 'GCM',\n",
    "                 'GCR', 'HINS1', 'HINS2', 'HINS3', 'HINS4', 'HINS5', 'HINS6',\n",
    "                 'HINS7', 'INTP', 'JWMNP', 'JWRIP', 'JWTR', 'LANX', 'MAR',\n",
    "                 'MARHD', 'MARHM', 'MARHT', 'MARHW', 'MARHYP', 'MIG', 'MIL',\n",
    "                 'MLPA', 'MLPB', 'MLPCD', 'MLPE', 'MLPFG', 'MLPH', 'MLPI',\n",
    "                 'MLPJ', 'MLPK', 'NWAB', 'NWAV', 'NWLA', 'NWLK', 'NWRE', 'OIP',\n",
    "                 'PAP', 'RELP', 'RETP', 'SCH', 'SCHG', 'SCHL', 'SEMP', 'SEX',\n",
    "                 'SSIP', 'SSP', 'WAGP', 'WKHP', 'WKL', 'WKW', 'WRK', 'YOEP',\n",
    "                 'ANC', 'ANC1P', 'ANC2P', 'DECADE', 'DIS', 'DRIVESP', 'ESP',\n",
    "                 'ESR', 'FHICOVP', 'FOD1P', 'FOD2P', 'HICOV', 'HISP', 'INDP',\n",
    "                 'JWAP', 'JWDP', 'LANP', 'MIGPUMA', 'MIGSP', 'MSP', 'NAICSP',\n",
    "                 'NATIVITY', 'NOP', 'OC', 'OCCP', 'PAOC', 'PERNP', 'PINCP',\n",
    "                 'POBP', 'POVPIP', 'POWPUMA', 'POWSP', 'PRIVCOV', 'PUBCOV',\n",
    "                 'QTRBIR', 'RAC1P', 'RAC2P', 'RAC3P', 'RACAIAN', 'RACAS',\n",
    "                 'RACBLK', 'RACNH', 'RACNUM', 'RACPI', 'RACSOR', 'RACWHT',\n",
    "                 'RC', 'SCIENGP', 'SCIENGRLP', 'SFN', 'SFR', 'SOCP', 'VPS',\n",
    "                 'WAOB', 'FAGEP', 'FANCP', 'FCITP', 'FCITWP', 'FCOWP',\n",
    "                 'FDDRSP', 'FDEARP', 'FDEYEP', 'FDISP', 'FDOUTP', 'FDPHYP',\n",
    "                 'FDRATP', 'FDRATXP', 'FDREMP', 'FENGP', 'FESRP', 'FFERP',\n",
    "                 'FFODP', 'FGCLP', 'FGCMP', 'FGCRP', 'FHINS1P', 'FHINS2P',\n",
    "                 'FHINS3C', 'FHINS3P', 'FHINS4C', 'FHINS4P', 'FHINS5C',\n",
    "                 'FHINS5P', 'FHINS6P', 'FHINS7P', 'FHISP', 'FINDP', 'FINTP',\n",
    "                 'FJWDP', 'FJWMNP', 'FJWRIP', 'FJWTRP', 'FLANP', 'FLANXP',\n",
    "                 'FMARHDP', 'FMARHMP', 'FMARHTP', 'FMARHWP', 'FMARHYP',\n",
    "                 'FMARP', 'FMIGP', 'FMIGSP', 'FMILPP', 'FMILSP', 'FOCCP',\n",
    "                 'FOIP', 'FPAP', 'FPERNP', 'FPINCP', 'FPOBP', 'FPOWSP',\n",
    "                 'FPRIVCOVP', 'FPUBCOVP', 'FRACP', 'FRELP', 'FRETP', 'FSCHGP',\n",
    "                 'FSCHLP', 'FSCHP', 'FSEMP', 'FSEXP', 'FSSIP', 'FSSP', 'FWAGP',\n",
    "                 'FWKHP', 'FWKLP', 'FWKWP', 'FWRKP', 'FYOEP', 'pwgtp1',\n",
    "                 'pwgtp2', 'pwgtp3', 'pwgtp4', 'pwgtp5', 'pwgtp6', 'pwgtp7',\n",
    "                 'pwgtp8', 'pwgtp9', 'pwgtp10', 'pwgtp11', 'pwgtp12',\n",
    "                 'pwgtp13', 'pwgtp14', 'pwgtp15', 'pwgtp16', 'pwgtp17',\n",
    "                 'pwgtp18', 'pwgtp19', 'pwgtp20', 'pwgtp21', 'pwgtp22',\n",
    "                 'pwgtp23', 'pwgtp24', 'pwgtp25', 'pwgtp26', 'pwgtp27',\n",
    "                 'pwgtp28', 'pwgtp29', 'pwgtp30', 'pwgtp31', 'pwgtp32',\n",
    "                 'pwgtp33', 'pwgtp34', 'pwgtp35', 'pwgtp36', 'pwgtp37',\n",
    "                 'pwgtp38', 'pwgtp39', 'pwgtp40', 'pwgtp41', 'pwgtp42',\n",
    "                 'pwgtp43', 'pwgtp44', 'pwgtp45', 'pwgtp46', 'pwgtp47',\n",
    "                 'pwgtp48', 'pwgtp49', 'pwgtp50', 'pwgtp51', 'pwgtp52',\n",
    "                 'pwgtp53', 'pwgtp54', 'pwgtp55', 'pwgtp56', 'pwgtp57',\n",
    "                 'pwgtp58', 'pwgtp59', 'pwgtp60', 'pwgtp61', 'pwgtp62',\n",
    "                 'pwgtp63', 'pwgtp64', 'pwgtp65', 'pwgtp66', 'pwgtp67',\n",
    "                 'pwgtp68', 'pwgtp69', 'pwgtp70', 'pwgtp71', 'pwgtp72',\n",
    "                 'pwgtp73', 'pwgtp74', 'pwgtp75', 'pwgtp76', 'pwgtp77',\n",
    "                 'pwgtp78', 'pwgtp79', 'pwgtp80')\n",
    "\n",
    "# If you change categorical_columns, target_column, or key_column, the training\n",
    "# transforms.json file also needs to change. \n",
    "categorical_columns = ['AGEP', 'COW', 'ESP', 'ESR', 'FOD1P', 'HINS4', 'INDP',\n",
    "                       'JWMNP', 'JWTR', 'MAR', 'POWPUMA', 'PUMA', 'RAC1P', 'SCHL',\n",
    "                       'SCIENGRLP', 'SEX', 'WKW']\n",
    "# WAGP will be the target column. Feel free to change the target to any of the other income like columns:\n",
    "# PERNP, INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, WAGP, etc\n",
    "target_column = 'WAGP'\n",
    "key_column = 'SERIALNO'\n",
    "\n",
    "all_raw_data = pd.read_csv(os.path.join(LOCAL_ROOT, 'ss14psd.csv'),\n",
    "                           header=None,\n",
    "                           names=csv_columns,\n",
    "                           dtype=str)\n",
    "\n",
    "#dtype = {'WAGP': np.}\n",
    "csv_columns_to_keep = [key_column] + [target_column] + categorical_columns\n",
    "\n",
    "# Keep only some of the columns\n",
    "all_data = all_raw_data[csv_columns_to_keep]\n",
    "\n",
    "# Replace whitespace with NaN\n",
    "all_data = all_data.replace('\\s+', np.nan, regex=True)\n",
    "\n",
    "# Replace NaN with 0\n",
    "all_data = all_data.fillna(0)\n",
    "\n",
    "# Also convert income unit from $1 to $1000.\n",
    "all_data = all_data.loc[ all_data[target_column] != ' ']\n",
    "all_data[target_column] = all_data[target_column].astype(float)/1000.0\n",
    "\n",
    "# Keep rows with a non-extream target range.\n",
    "all_data = all_data.loc[ np.logical_and(all_data[target_column] > 10.0, all_data[target_column] < 150.0) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234321)\n",
    "random_numbers = np.random.rand(len(all_data))\n",
    "\n",
    "# slit all_data into %80, %10, %10 percent sets.\n",
    "train_data = all_data[random_numbers < 0.8]\n",
    "eval_data = all_data[ np.logical_and(random_numbers >= 0.8, random_numbers < 0.9)]\n",
    "predict_data = all_data[random_numbers >= 0.9]\n",
    "\n",
    "# remove target column from prediction set\n",
    "del predict_data[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schema file is used to describe each column of the csv files. It is assumed that the train, eval, and prediction csv files all have the same schema, but the prediction file has a missing target column. The format of the  schema file is a valid BigQuery table schema file. This allows BigQuery to be used later in cloud preprocessing. Only 3 BigQuery types are supported: STRING (for categorical columns) and INTEGER and FLOAT (for numerical columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the data to a file.\n",
    "train_data.to_csv(os.path.join(LOCAL_ROOT, 'train_data.csv'),\n",
    "                  header=False,\n",
    "                  index=False)\n",
    "eval_data.to_csv(os.path.join(LOCAL_ROOT, 'eval_data.csv'),\n",
    "                  header=False,\n",
    "                  index=False)\n",
    "predict_data.to_csv(os.path.join(LOCAL_ROOT, 'predict_data.csv'),\n",
    "                  header=False,\n",
    "                  index=False)\n",
    "\n",
    "# Also write a BigQuery schema file for the csv files.\n",
    "schema = (\n",
    "  [\n",
    "    {'name': key_column, 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "    {'name': target_column, 'type': 'FLOAT', 'mode': 'NULLABLE'}\n",
    "  ] + [{'name': name, 'type': 'STRING', 'mode': 'NULLABLE'} for name in categorical_columns]\n",
    ")\n",
    "file_io.write_string_to_file(os.path.join(LOCAL_ROOT, 'schema.json'),\n",
    "                             json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"local_preprocessing\"></a>\n",
    "Local preprocessing starting from csv files\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -fr {LOCAL_ROOT}/preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_csv = ml.CsvDataSet(\n",
    "  file_pattern=os.path.join(LOCAL_ROOT, 'train_data.csv'),\n",
    "  schema_file=os.path.join(LOCAL_ROOT, 'schema.json'))\n",
    "eval_csv = ml.CsvDataSet(\n",
    "  file_pattern=os.path.join(LOCAL_ROOT, 'eval_data.csv'),\n",
    "  schema_file=os.path.join(LOCAL_ROOT, 'schema.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local preprocessing.\n",
      "Local preprocessing done.\n"
     ]
    }
   ],
   "source": [
    "sd.local_preprocess(\n",
    "  dataset=train_csv,\n",
    "  output_dir=os.path.join(LOCAL_ROOT, 'preprocess'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of preprocessing is a numerical_analysis file that contains analysis from the numerical columns, and a vocab file from each categorical column. The files produced by preprocessing are consumed in training, and you should not have to worry about these files. Just for fun, lets look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_analysis.json  vocab_HINS4.csv    vocab_RAC1P.csv\r\n",
      "schema.json\t\t vocab_INDP.csv     vocab_SCHL.csv\r\n",
      "vocab_AGEP.csv\t\t vocab_JWMNP.csv    vocab_SCIENGRLP.csv\r\n",
      "vocab_COW.csv\t\t vocab_JWTR.csv     vocab_SERIALNO.csv\r\n",
      "vocab_ESP.csv\t\t vocab_MAR.csv\t    vocab_SEX.csv\r\n",
      "vocab_ESR.csv\t\t vocab_POWPUMA.csv  vocab_WKW.csv\r\n",
      "vocab_FOD1P.csv\t\t vocab_PUMA.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls  {LOCAL_ROOT}/preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"local_training\"></a>\n",
    "Local Training\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files in the output folder of preprocessing are consumed by the trainer. Training requires a transform config file to describe what transforms to apply on the data. The key and target transform are the only required transform, a default transform will be applied to every other column if it is not listed in the transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transforms = {\n",
    "  \"WAGP\": {\"transform\": \"target\"},\n",
    "  \"SERIALNO\": {\"transform\": \"key\"},\n",
    "  \"AGEP\": {\"transform\": \"embedding\", \"embedding_dim\": 2}, # age\n",
    "  \"COW\": {\"transform\": \"one_hot\"}, # class of worker\n",
    "  \"ESP\": {\"transform\": \"embedding\", \"embedding_dim\": 2}, # Employment status of parents\n",
    "  \"ESR\": {\"transform\": \"one_hot\"}, #Employment status\n",
    "  \"FOD1P\": {\"transform\": \"embedding\", \"embedding_dim\": 3}, #field of degree\n",
    "  \"HINS4\": {\"transform\": \"one_hot\"}, #Medicaid\n",
    "  \"INDP\": {\"transform\": \"embedding\", \"embedding_dim\": 5}, # industry\n",
    "  \"JWMNP\": {\"transform\": \"hash_embedding\", \"hash_bucket_size\": 20, \"embedding_dim\": 2}, # travel time to work\n",
    "  \"JWTR\": {\"transform\": \"one_hot\"}, #Means of transportation to work\n",
    "  \"MAR\": {\"transform\": \"one_hot\"}, #Marital status\n",
    "  \"POWPUMA\": {\"transform\": \"hash_one_hot\", \"hash_bucket_size\": 4}, #Place of work\n",
    "  \"PUMA\": {\"transform\": \"one_hot\"}, #area code\n",
    "  \"RAC1P\": {\"transform\": \"one_hot\"}, #race code\n",
    "  \"SCHL\": {\"transform\": \"one_hot\"}, #school\n",
    "  \"SCIENGRLP\": {\"transform\": \"one_hot\"}, # Science\n",
    "  \"SEX\": {\"transform\": \"one_hot\"},\n",
    "  \"WKW\": {\"transform\": \"one_hot\"}, #Weeks worked\n",
    "}\n",
    "file_io.write_string_to_file(os.path.join(LOCAL_ROOT, 'transforms.json'),\n",
    "                             json.dumps(transforms, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -fr {LOCAL_ROOT}/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local training.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6670658d50>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6670658d50>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:322: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:322: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into ./census_regression_workspace/training/train/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into ./census_regression_workspace/training/train/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1927.57, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1927.57, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-02-23-18:06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-02-23-18:06:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [1/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [1/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [2/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [2/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [3/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [3/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2017-02-23-18:06:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2017-02-23-18:06:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 1565.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 1565.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Validation (step 100): loss = 1565.84, global_step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Validation (step 100): loss = 1565.84, global_step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 20.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 20.9157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 407.338, step = 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 407.338, step = 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 247.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 247.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 318.402, step = 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 318.402, step = 201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 235.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 235.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 291.46, step = 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 291.46, step = 301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 236.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 236.687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 192.56, step = 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 192.56, step = 401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 240.448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 240.448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 367.718, step = 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 367.718, step = 501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 189.296, step = 601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 189.296, step = 601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 232.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 232.624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 219.035, step = 701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 219.035, step = 701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 243.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 243.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 386.178, step = 801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 386.178, step = 801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 224.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 224.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 324.573, step = 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 324.573, step = 901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 256.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 256.383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 292.532, step = 1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 292.532, step = 1001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 234.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 234.221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 200.719, step = 1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 200.719, step = 1101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 228.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 228.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 200.692, step = 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 200.692, step = 1201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 237.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 237.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 276.001, step = 1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 276.001, step = 1301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 235.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 235.607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 152.407, step = 1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 152.407, step = 1401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 239.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 239.374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 265.338, step = 1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 265.338, step = 1501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 231.588, step = 1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 231.588, step = 1601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 238.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 238.814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 288.931, step = 1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 288.931, step = 1701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 249.24, step = 1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 249.24, step = 1801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 230.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 230.892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 227.053, step = 1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 227.053, step = 1901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 2000 into ./census_regression_workspace/training/train/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 2000 into ./census_regression_workspace/training/train/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 149.588.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 149.588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-02-23-18:06:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-02-23-18:06:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [1/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [1/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [2/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [2/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [3/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [3/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2017-02-23-18:06:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2017-02-23-18:06:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, loss = 399.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, loss = 399.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./census_regression_workspace/training/train/export/intermediate_evaluation_models/1487873178087/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./census_regression_workspace/training/train/export/intermediate_evaluation_models/1487873178087/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./census_regression_workspace/training/train/export/intermediate_prediction_models/1487873181183/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./census_regression_workspace/training/train/export/intermediate_prediction_models/1487873181183/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local training done.\n"
     ]
    }
   ],
   "source": [
    "sd.local_train(\n",
    "  train_dataset=train_csv,\n",
    "  eval_dataset=eval_csv,\n",
    "  transforms=os.path.join(LOCAL_ROOT, 'transforms.json'),\n",
    "  preprocess_output_dir=os.path.join(LOCAL_ROOT, 'preprocess'),\n",
    "  output_dir=os.path.join(LOCAL_ROOT, 'training'),\n",
    "  model_type='dnn_regression',\n",
    "  max_steps=2000,\n",
    "  layer_sizes=[5, 5, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one run of the notebook, the loss on the evaluation set was ~400 (or a RMSE of ~$20,000). For you the loss might be different. Let us check if this loss is decent by comparing with a model that simply returns the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560.563578606\n"
     ]
    }
   ],
   "source": [
    "eval_mean = eval_data[target_column].mean()\n",
    "loss = ((eval_data[target_column] -  eval_mean)*(eval_data[target_column] -  eval_mean)).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one run of the notebook, the eval loss using the mean as a prediction was ~560. Our model is better than simply guessing the mean, but not by much. Try playing with the transforms, or changing the layer sizes, learning rate, or training steps to reduce the loss. Try adding other columns from the dataset to the model, or try predicting a different income-line column. You might find that predicting income from just demographic-like information is challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"local_prediction\"></a>\n",
    "Local prediction\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local predict uses the model produced by training. The input data can be a csv string or Pandas DataFrame, but the schema must match the data set used for training, except the target column is missing. That is, if the training dataset had the values \"id,target,value1,value2\", the prediction data must be in the form \"id,value1,value2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local prediction.\n",
      "Local prediction done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_from_input</th>\n",
       "      <th>predicted_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>490</td>\n",
       "      <td>20.630501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1225</td>\n",
       "      <td>59.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1226</td>\n",
       "      <td>20.864326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key_from_input  predicted_target\n",
       "0             490         20.630501\n",
       "1            1225         59.453800\n",
       "2            1226         20.864326"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.local_predict(\n",
    "  training_ouput_dir=os.path.join(LOCAL_ROOT, 'training'),\n",
    "  data=['490,64,2,0,1,0,2,8090,015,01,1,00590,00500,1,18,0,2,1',\n",
    "        '1225,32,5,0,4,5301,2,9680,015,01,1,00100,00100,1,21,2,1,1',\n",
    "        '1226,30,1,0,1,0,2,8680,020,01,1,00100,00100,1,16,0,2,1']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local prediction.\n",
      "Local prediction done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_from_input</th>\n",
       "      <th>predicted_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>490</td>\n",
       "      <td>20.630501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1225</td>\n",
       "      <td>59.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1226</td>\n",
       "      <td>20.864326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key_from_input  predicted_target\n",
       "0             490         20.630501\n",
       "1            1225         59.453800\n",
       "2            1226         20.864326"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.local_predict(\n",
    "  training_ouput_dir=os.path.join(LOCAL_ROOT, 'training'),\n",
    "  data=pd.DataFrame(\n",
    "    [[490,64,2,0,1,0,2,8090,\"015\",\"01\",1,\"00590\",\"00500\",1,18,0,2,1],\n",
    "     [1225,32,5,0,4,5301,2,9680,\"015\",\"01\",1,\"00100\",\"00100\",1,21,2,1,1],\n",
    "     [1226,30,1,0,1,0,2,8680,\"020\",\"01\",1,\"00100\",\"00100\",1,16,0,2,1]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"local_batch_prediction\"></a>\n",
    "Local batch prediction\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local batch prediction runs prediction on batched input data. This is ideal if the input dataset is very large or you have limited available main memory. However, for very large datasets, it is better to run batch prediction using the Google Cloud Machine Learning Engine services. Two output formats are supported, csv and json. The output may also be shardded. Another feature of batch prediction is the option to run evaluation--prediction on data that contains the target column. Like local_predict, the input data must batch the schema used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -fr {LOCAL_ROOT}/predict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local batch prediction.\n",
      "Local batch prediction done.\n"
     ]
    }
   ],
   "source": [
    "sd.local_batch_predict(\n",
    "  training_ouput_dir=os.path.join(LOCAL_ROOT, 'training'),\n",
    "  prediction_input_file=os.path.join(LOCAL_ROOT, 'eval_data.csv'),\n",
    "  output_dir=os.path.join(LOCAL_ROOT, 'predict_out'),\n",
    "  output_format='json',\n",
    "  mode='evaluation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors-00000-of-00001.txt  predictions-00000-of-00001.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls {LOCAL_ROOT}/predict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat {LOCAL_ROOT}/predict_out/errors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"target_from_input\": 40.0,\"predicted_target\": 26.240245819091797,\"key_from_input\": \"11735\"}\r\n",
      "{\"target_from_input\": 20.0,\"predicted_target\": 35.266021728515625,\"key_from_input\": \"12794\"}\r\n",
      "{\"target_from_input\": 70.0,\"predicted_target\": 34.8465461730957,\"key_from_input\": \"23402\"}\r\n",
      "{\"target_from_input\": 45.0,\"predicted_target\": 54.92388153076172,\"key_from_input\": \"30351\"}\r\n",
      "{\"target_from_input\": 32.0,\"predicted_target\": 45.367523193359375,\"key_from_input\": \"30470\"}\r\n",
      "{\"target_from_input\": 17.0,\"predicted_target\": 35.007110595703125,\"key_from_input\": \"32995\"}\r\n",
      "{\"target_from_input\": 39.0,\"predicted_target\": 27.419919967651367,\"key_from_input\": \"40409\"}\r\n",
      "{\"target_from_input\": 35.0,\"predicted_target\": 36.686256408691406,\"key_from_input\": \"49363\"}\r\n",
      "{\"target_from_input\": 38.0,\"predicted_target\": 57.945594787597656,\"key_from_input\": \"54409\"}\r\n",
      "{\"target_from_input\": 28.0,\"predicted_target\": 34.59382247924805,\"key_from_input\": \"68049\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head {LOCAL_ROOT}/predict_out/predictions-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run batch prediction again this time using data that does not have a target column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -fr {LOCAL_ROOT}/predict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local batch prediction.\n",
      "Local batch prediction done.\n"
     ]
    }
   ],
   "source": [
    "sd.local_batch_predict(\n",
    "  training_ouput_dir=os.path.join(LOCAL_ROOT, 'training'),\n",
    "  prediction_input_file=os.path.join(LOCAL_ROOT, 'predict_data.*'),\n",
    "  output_dir=os.path.join(LOCAL_ROOT, 'predict_out'),\n",
    "  output_format='csv',\n",
    "  mode='prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv_header.json  errors-00000-of-00001.txt  predictions-00000-of-00001.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {LOCAL_ROOT}/predict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"type\": \"STRING\", \r\n",
      "    \"mode\": \"NULLABLE\", \r\n",
      "    \"name\": \"key_from_input\"\r\n",
      "  }, \r\n",
      "  {\r\n",
      "    \"type\": \"STRING\", \r\n",
      "    \"mode\": \"NULLABLE\", \r\n",
      "    \"name\": \"predicted_target\"\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!cat {LOCAL_ROOT}/predict_out/csv_header.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2892,55.0839347839\r\n",
      "3137,57.2630462646\r\n",
      "10412,30.8309402466\r\n",
      "14981,8.9282541275\r\n",
      "32166,12.1347923279\r\n",
      "32166,28.7190818787\r\n",
      "32881,50.8943595886\r\n",
      "36459,25.4952888489\r\n",
      "37563,38.2368469238\r\n",
      "42471,24.4276676178\r\n"
     ]
    }
   ],
   "source": [
    "!head {LOCAL_ROOT}/predict_out/predictions-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Things up\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As everything was written to LOCAL_ROOT, we can simply remove this folder. If you want to delete those files, uncomment and run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!rm -fr {LOCAL_ROOT}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
