{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Batch prediction and evaluation are very similar. They are based on DataFlow pipeline and CloudML provides Evaluate and Prediction DataFlow transform. Datalab can generate DataFlow pipeline code template for you, just like Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"%mlalpha evaluate\" to generate input cell. After fill in the required fields, it becomes:\n",
    "```\n",
    "%%mlalpha evaluate\n",
    "preprocessed_eval_data_path: /content/datalab/tmp/ml/census/preprocessed/features_eval*\n",
    "metadata_path: /content/datalab/tmp/ml/census/preprocessed/metadata.yaml\n",
    "model_dir: /content/datalab/tmp/ml/census/model/model\n",
    "output_dir: /content/datalab/tmp/ml/census/evaluate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7fae2b91bcd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%mlalpha evaluate\n",
    "preprocessed_eval_data_path: /content/datalab/tmp/ml/census/preprocessed/features_eval*\n",
    "metadata_path: /content/datalab/tmp/ml/census/preprocessed/metadata.yaml\n",
    "model_dir: /content/datalab/tmp/ml/census/model/model\n",
    "output_dir: /content/datalab/tmp/ml/census/evaluate\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.analysis as analysis\n",
    "import google.cloud.ml.io as io\n",
    "import json\n",
    "import os\n",
    "\n",
    "# defines\n",
    "def extract_values((example, prediction)):\n",
    "  import tensorflow as tf\n",
    "  tf_example = tf.train.Example()\n",
    "  tf_example.ParseFromString(example.values()[0])\n",
    "  feature_map = tf_example.features.feature\n",
    "  values = {'target': feature_map['target'].float_list.value[0]}\n",
    "  values.update(prediction)\n",
    "  return values\n",
    "\n",
    "OUTPUT_DIR = '/content/datalab/tmp/ml/census/evaluate'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# evaluation\n",
    "\n",
    "eval_features = (pipeline | 'ReadEval' >> io.LoadFeatures('/content/datalab/tmp/ml/census/preprocessed/features_eval*'))\n",
    "trained_model = pipeline | 'LoadModel' >> io.LoadModel('/content/datalab/tmp/ml/census/model/model')\n",
    "evaluations = (eval_features | 'Evaluate' >> ml.Evaluate(trained_model) |\n",
    "    beam.Map('ExtractEvaluationResults', extract_values))\n",
    "eval_data_sink = beam.io.TextFileSink(os.path.join(OUTPUT_DIR, 'eval'), shard_name_template='')\n",
    "evaluations | beam.io.textio.WriteToText(os.path.join(OUTPUT_DIR, 'eval'), shard_name_template='')\n",
    "\n",
    "# analysis\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the output eval file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'score': [62744.8359375], 'target': 172300.0, u'key': [172300.0]}\r\n",
      "{u'score': [62741.1328125], 'target': 28000.0, u'key': [28000.0]}\r\n",
      "{u'score': [54228.66015625], 'target': 37300.0, u'key': [37300.0]}\r\n",
      "{u'score': [47622.3984375], 'target': 42400.0, u'key': [42400.0]}\r\n",
      "{u'score': [61382.60546875], 'target': 100000.0, u'key': [100000.0]}\r\n",
      "{u'score': [58404.56640625], 'target': 53000.0, u'key': [53000.0]}\r\n",
      "{u'score': [60505.54296875], 'target': 63300.0, u'key': [63300.0]}\r\n",
      "{u'score': [64454.1484375], 'target': 56200.0, u'key': [56200.0]}\r\n",
      "{u'score': [64122.703125], 'target': 132000.0, u'key': [132000.0]}\r\n",
      "{u'score': [53015.11328125], 'target': 55000.0, u'key': [55000.0]}\r\n"
     ]
    }
   ],
   "source": [
    "!head /content/datalab/tmp/ml/census/evaluate/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a pipeline that runs in cloud, simply add --cloud to \"%mlalpha evaluate\". Also all paths need to be GCS paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = 'gs://' + datalab_project_id() + '-sampledata'\n",
    "eval_data_path = os.path.join(bucket, 'census', 'preprocessed', 'features_eval')\n",
    "metadata_path = os.path.join(bucket, 'census', 'preprocessed', 'metadata.yaml')\n",
    "model_path = os.path.join(bucket, 'census', 'trained', 'model')\n",
    "output_dir = os.path.join(bucket, 'census', 'evaluate')\n",
    "eval_file = os.path.join(output_dir, 'eval*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%ml evaluate --cloud\n",
    "preprocessed_eval_data_path: $eval_data_path\n",
    "metadata_path: $metadata_path\n",
    "model_dir: $model_path\n",
    "output_dir: $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run the generated code, you can go to Developer Console to see the DataFlow job: https://pantheon.corp.google.com/dataflow (and select the right project). Also you can check the results as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'score': [62152.2109375], 'target': 172300.0, u'key': ['705']}\r\n",
      "{u'score': [50811.1875], 'target': 28000.0, u'key': ['1204']}\r\n",
      "{u'score': [41454.01953125], 'target': 37300.0, u'key': ['1758']}\r\n",
      "{u'score': [70818.65625], 'target': 42400.0, u'key': ['2534']}\r\n",
      "{u'score': [63665.48046875], 'target': 100000.0, u'key': ['3172']}\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat $eval_file | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
