{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training requires a tarball python package that includes your training program based on TensorFlow. In this example, we will use <a href=\"http://tflearn.org/\">tf.learn</a> to implement the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Package\n",
    "\n",
    "You can use existing tarball package (locally or in GCS), or use your own tarball package. You can define a python module use \"%%mlalpha module\". In the following two cells, we will define two python modules: \"iris\" and \"task\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha module --name iris\n",
    "\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import google.cloud.ml.features as features\n",
    "\n",
    "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\n",
    "  \"\"\"Creates readers and queues for reading example protos.\"\"\"\n",
    "  files = []\n",
    "  for e in input_files:\n",
    "    for path in e.split(','):\n",
    "      files.extend(file_io.get_matching_files(path))\n",
    "  thread_count = multiprocessing.cpu_count()\n",
    "\n",
    "  # The minimum number of instances in a queue from which examples are drawn\n",
    "  # randomly. The larger this number, the more randomness at the expense of\n",
    "  # higher memory requirements.\n",
    "  min_after_dequeue = 1000\n",
    "\n",
    "  # When batching data, the queue's capacity will be larger than the batch_size\n",
    "  # by some factor. The recommended formula is (num_threads + a small safety\n",
    "  # margin). For now, we use a single thread for reading, so this can be small.\n",
    "  queue_size_multiplier = thread_count + 3\n",
    "\n",
    "  # Convert num_epochs == 0 -> num_epochs is None, if necessary\n",
    "  num_epochs = num_epochs or None\n",
    "\n",
    "  # Build a queue of the filenames to be read.\n",
    "  filename_queue = tf.train.string_input_producer(files, num_epochs, shuffle)\n",
    "\n",
    "  options = tf.python_io.TFRecordOptions(\n",
    "      compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "  example_id, encoded_example = tf.TFRecordReader(options=options).read_up_to(\n",
    "      filename_queue, batch_size)\n",
    "\n",
    "  if shuffle:\n",
    "    capacity = min_after_dequeue + queue_size_multiplier * batch_size\n",
    "    return tf.train.shuffle_batch(\n",
    "        [example_id, encoded_example],\n",
    "        batch_size,\n",
    "        capacity,\n",
    "        min_after_dequeue,\n",
    "        enqueue_many=True,\n",
    "        num_threads=thread_count)\n",
    "\n",
    "  else:\n",
    "    capacity = queue_size_multiplier * batch_size\n",
    "    return tf.train.batch(\n",
    "        [example_id, encoded_example],\n",
    "        batch_size,\n",
    "        capacity=capacity,\n",
    "        enqueue_many=True,\n",
    "        num_threads=thread_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define \"task\" module. \"--main\" indicates this is the entry point of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha module --name task --main\n",
    "\n",
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "from . import iris\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import metrics as metrics_lib\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.session_bundle import gc\n",
    "from tensorflow.contrib.session_bundle import manifest_pb2\n",
    "\n",
    "import google.cloud.ml as ml\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# This determines a single column that is used to obtain features\n",
    "# after parsing TF.EXamples,\n",
    "FEATURES_KEY = 'measurements'\n",
    "\n",
    "# The following keys determine columns from the parsed Examples to be included\n",
    "# in the output.\n",
    "TARGET_KEY = 'species'\n",
    "KEY_ = 'key'\n",
    "\n",
    "# This is used to map to unparsed tf.Examples so we can output them.\n",
    "EXAMPLES_KEY = '_EXAMPLES_'\n",
    "\n",
    "\n",
    "def _get_input_fn(args, mode):\n",
    "  \"\"\"Input function used by the experiment.\"\"\"\n",
    "\n",
    "  def input_fn():\n",
    "    \"\"\"Estimator `input_fn`.\n",
    "\n",
    "    Returns:\n",
    "      A tuple of:\n",
    "      - Dictionary of string feature name to `Tensor`.\n",
    "      - `Tensor` of target labels.\n",
    "    \"\"\"\n",
    "    metadata = ml.features.FeatureMetadata.get_metadata(args.metadata_path)\n",
    "\n",
    "    if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "      # Generate placeholders for the examples.\n",
    "      examples = tf.placeholder(\n",
    "          dtype=tf.string,\n",
    "          shape=(None,),\n",
    "          name='input_example')\n",
    "\n",
    "      parsed = ml.features.FeatureMetadata.parse_features(metadata, examples)\n",
    "      parsed[EXAMPLES_KEY] = examples\n",
    "      parsed[TARGET_KEY] = tf.ones_like(parsed[TARGET_KEY]) * (-1)\n",
    "      return parsed, None\n",
    "\n",
    "    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "      _, examples = iris.read_examples(\n",
    "          args.train_data_paths, args.batch_size, shuffle=True)\n",
    "    else:\n",
    "      _, examples = iris.read_examples(\n",
    "          args.eval_data_paths, args.eval_batch_size, shuffle=False)\n",
    "\n",
    "    parsed = ml.features.FeatureMetadata.parse_features(metadata, examples)\n",
    "\n",
    "    s = parsed.pop(TARGET_KEY)\n",
    "    return parsed, s\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "def _generic_signature_fn(examples, features, predictions):\n",
    "  \"\"\"Create a generic signature function with input and output signatures.\"\"\"\n",
    "  # Mark the outputs.\n",
    "  predicted_index = tf.argmax(predictions, 1)\n",
    "  predicted_label = tf.contrib.lookup.index_to_string(predicted_index,\n",
    "                                                      mapping=[\"setosa\", \"versicolor\", \"virginica\"],\n",
    "                                                      default_value=\"UNKNOWN\")\n",
    "  outputs = {'scores': predictions.name,\n",
    "             'key': tf.squeeze(features[KEY_]).name,\n",
    "             'predicted_index': predicted_index.name,\n",
    "             'predicted_label': predicted_label.name}\n",
    "\n",
    "  inputs = {'examples': features[EXAMPLES_KEY].name}\n",
    "\n",
    "  tf.add_to_collection('outputs', json.dumps(outputs))\n",
    "  tf.add_to_collection('inputs', json.dumps(inputs))\n",
    "\n",
    "  input_signature = manifest_pb2.Signature()\n",
    "  output_signature = manifest_pb2.Signature()\n",
    "\n",
    "  for name, tensor_name in outputs.iteritems():\n",
    "    output_signature.generic_signature.map[name].tensor_name = tensor_name\n",
    "\n",
    "  for name, tensor_name in inputs.iteritems():\n",
    "    input_signature.generic_signature.map[name].tensor_name = tensor_name\n",
    "\n",
    "  return None, {'inputs': input_signature,\n",
    "                'outputs': output_signature}\n",
    "\n",
    "\n",
    "def _get_export_monitor(args, output_dir):\n",
    "  \"\"\"Create an export monitor.\"\"\"\n",
    "  export_input_fn = _get_input_fn(args, tf.contrib.learn.ModeKeys.INFER)\n",
    "\n",
    "  class ExportWithMetadataMonitor(tf.contrib.learn.monitors.ExportMonitor):\n",
    "\n",
    "    def every_n_step_end(self, step, outputs):\n",
    "      # Don't export every n steps.\n",
    "      pass\n",
    "\n",
    "    def end(self, session=None):\n",
    "      super(ExportWithMetadataMonitor, self).end(session=session)\n",
    "      # Save a copy of the metadata file used for this model with the exported\n",
    "      # model, so that online and batch prediction can use it.\n",
    "\n",
    "      subprocess.check_call([\n",
    "          'gsutil', '-q', 'cp', args.metadata_path,\n",
    "          os.path.join(output_dir, 'model', 'metadata.yaml')])\n",
    "      # Copy the final model to the model/ directory, so that we can find it.\n",
    "      def parser(path):\n",
    "        match = re.match('^' + self.export_dir + '/(\\\\d+)$', path.path)\n",
    "        if not match:\n",
    "          return None\n",
    "        return path._replace(export_version=int(match.group(1)))\n",
    "\n",
    "      dir_list = gc.get_paths(self.export_dir, parser=parser)\n",
    "      subprocess.check_call(['gsutil', '-q', 'cp', '-r',\n",
    "                             dir_list[-1].path + '/*',\n",
    "                             os.path.join(output_dir, 'model')])\n",
    "\n",
    "  local_output_dir = tempfile.mkdtemp()\n",
    "  return ExportWithMetadataMonitor(\n",
    "      input_fn=export_input_fn,\n",
    "      input_feature_key=EXAMPLES_KEY,\n",
    "      every_n_steps=400,\n",
    "      export_dir=os.path.join(local_output_dir, 'export'),\n",
    "      signature_fn=_generic_signature_fn)\n",
    "\n",
    "\n",
    "def _get_experiment_fn(args):\n",
    "  \"\"\"Create the experiment function.\"\"\"\n",
    "\n",
    "  def _experiment_fn(output_dir):\n",
    "    \"\"\"Experiment function used by learn_runner to run training/eval/etc.\n",
    "\n",
    "    Args:\n",
    "      output_dir: String path of directory to use for outputs (model\n",
    "        checkpoints, summaries, etc).\n",
    "\n",
    "    Returns:\n",
    "      tf.learn `Experiment`.\n",
    "    \"\"\"\n",
    "    config = tf.contrib.learn.RunConfig()\n",
    "    # Write checkpoints more often for more granular evals, since the toy data\n",
    "    # set is so small and simple. Most normal use cases should not set this and\n",
    "    # just use the default (600).\n",
    "    config.save_checkpoints_secs = 120\n",
    "\n",
    "    # Specify that all features have real-valued data\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(FEATURES_KEY,\n",
    "                                                            dimension=4)]\n",
    "\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "\n",
    "    classifier = tf.contrib.learn.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[args.hidden],\n",
    "        n_classes=NUM_CLASSES,\n",
    "        config=config,\n",
    "        model_dir=train_dir,\n",
    "        optimizer=tf.train.AdamOptimizer(\n",
    "            args.learning_rate, epsilon=args.epsilon))\n",
    "\n",
    "    train_monitors = [_get_export_monitor(args, output_dir)]\n",
    "\n",
    "    streaming_accuracy = metrics_lib.streaming_accuracy\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        estimator=classifier,\n",
    "        train_input_fn=_get_input_fn(args, tf.contrib.learn.ModeKeys.TRAIN),\n",
    "        eval_input_fn=_get_input_fn(args, tf.contrib.learn.ModeKeys.EVAL),\n",
    "        train_steps=args.max_steps,\n",
    "        train_monitors=train_monitors,\n",
    "        eval_metrics={\n",
    "            ('accuracy', 'classes'): streaming_accuracy,\n",
    "            ('training/hptuning/metric', 'classes'): streaming_accuracy\n",
    "        })\n",
    "  return _experiment_fn\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--train_data_paths', type=str, action='append')\n",
    "  parser.add_argument('--eval_data_paths', type=str, action='append')\n",
    "  parser.add_argument('--metadata_path', type=str)\n",
    "  parser.add_argument('--output_path', type=str)\n",
    "  parser.add_argument('--max_steps', type=int, default=5000)\n",
    "  parser.add_argument('--hidden', type=int, default=20)\n",
    "  parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "  parser.add_argument('--epsilon', type=float, default=0.0005)\n",
    "  parser.add_argument('--batch_size', type=int, default=30)\n",
    "  parser.add_argument('--eval_batch_size', type=int, default=30)\n",
    "  return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "  # First find out if there's a task value on the environment variable.\n",
    "  # If there is none or it is empty define a default one.\n",
    "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
    "\n",
    "  args = parse_arguments()\n",
    "\n",
    "\n",
    "  trial = task_data.get('trial')\n",
    "  if trial is not None:\n",
    "    output_dir = os.path.join(args.output_path, trial)\n",
    "  else:\n",
    "    output_dir = args.output_path\n",
    "\n",
    "  learn_runner.run(\n",
    "      experiment_fn=_get_experiment_fn(args),\n",
    "      output_dir=output_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training. Note that we specifies only program args in cell. This is equivelant to starting one role (master) for training. If you want to simulate distribiuted training, we can specify parameter_server_count and worker_count and Datalab will start three processes (master, worker, ps) to simulate cloud training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 589, in batch<br/>master:     _enqueue(queue, tensor_list, num_threads, enqueue_many)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 492, in _enqueue<br/>master:     enqueue_ops = [queue.enqueue_many(tensor_list)] * threads<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 371, in enqueue_many<br/>master:     self._queue_ref, vals, name=scope)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1018, in _queue_enqueue_many<br/>master:     name=name)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 756, in apply_op<br/>master:     op_def=op_def)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op<br/>master:     original_op=self._default_original_op, op_def=op_def)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__<br/>master:     self._traceback = _extract_stack()<br/>master: <br/>master: CancelledError (see above for traceback): Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderReadUpTo, ReaderReadUpTo:1)]]<br/>master: <br/>master: INFO:tensorflow:Saving evaluation summary for 2000 step: loss = 0.38825, training/hptuning/metric = 0.866667, accuracy = 0.866667<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%mlalpha train\n",
    "args:\n",
    "  train_data_paths: /content/datalab/tmp/ml/iris/preprocessed/features_train.tfrecord.gz\n",
    "  eval_data_paths: /content/datalab/tmp/ml/iris/preprocessed/features_eval.tfrecord.gz\n",
    "  metadata_path: /content/datalab/tmp/ml/iris/preprocessed/metadata.yaml\n",
    "  output_path: /content/datalab/tmp/ml/iris/model\n",
    "  max_steps: 2000\n",
    "  hidden: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after training is completed, you can increment \"max_steps\" and run it again. Training will resume from previous checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output of the training. \"model\" subdir includes the exported model. There are also tf events files under \"train\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  train\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/datalab/tmp/ml/iris/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start TensorBoard to view training results. Check the \"loss\" event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 11606. Click <a href=\"/_proxy/43630/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir /content/datalab/tmp/ml/iris/model/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the tensorboard server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 11606"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train another one for fun (with learning_rate equal to 0.001). learning_rate is an arg defined in training program in the package and default value is 0.01.\n",
    "\n",
    "Instead of running the modules defined by \"%%mlalpha modules\" directly, we will package the modules first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package created at /content/datalab/tmp/ml/iris/trainer-0.1.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "%%mlalpha package --out /content/datalab/tmp/ml/iris/ --name trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the package explicitly by package_uris. This time we specify 'parameter_server_count' or 'worker_count' so it will be a distributed training. In the cloud run, it will be equivalent to ps, worker, and master running in their own VMs. <br>\n",
    "Note that in the training output you have three log links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/worker\" target=\"_blank\">worker log</a>&nbsp;&nbsp;<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;<a href=\"/_nocachecontent/ps\" target=\"_blank\">ps log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 589, in batch<br/>master:     _enqueue(queue, tensor_list, num_threads, enqueue_many)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 492, in _enqueue<br/>master:     enqueue_ops = [queue.enqueue_many(tensor_list)] * threads<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 371, in enqueue_many<br/>master:     self._queue_ref, vals, name=scope)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1018, in _queue_enqueue_many<br/>master:     name=name)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 756, in apply_op<br/>master:     op_def=op_def)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op<br/>master:     original_op=self._default_original_op, op_def=op_def)<br/>master:   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__<br/>master:     self._traceback = _extract_stack()<br/>master: <br/>master: CancelledError (see above for traceback): Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderReadUpTo, ReaderReadUpTo:1)]]<br/>master: <br/>master: INFO:tensorflow:Saving evaluation summary for 2502 step: loss = 0.160581, training/hptuning/metric = 0.866667, accuracy = 0.866667<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mlalpha train\n",
    "package_uris: /content/datalab/tmp/ml/iris/trainer-0.1.tar.gz\n",
    "python_module: trainer.task\n",
    "worker_count: 1\n",
    "parameter_server_count: 1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/datalab/tmp/ml/iris/preprocessed/features_train.tfrecord.gz\n",
    "  eval_data_paths:\n",
    "    - /content/datalab/tmp/ml/iris/preprocessed/features_eval.tfrecord.gz\n",
    "  metadata_path: /content/datalab/tmp/ml/iris/preprocessed/metadata.yaml\n",
    "  output_path: /content/datalab/tmp/ml/iris/model_lr\n",
    "  max_steps: 2500\n",
    "  learning_rate: 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Training\n",
    "\n",
    "Cloud training is similar but with \"--cloud\" flag, and use all GCS paths instead of local paths. <br>\n",
    "We will use the preprocessed files created by cloud preprocessing in previous \"Preprocess\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = 'gs://' + datalab_project_id() + '-sampledata'\n",
    "package_path = os.path.join(bucket, 'iris', 'model', 'trainer-0.1.tar.gz')\n",
    "train_data_path = os.path.join(bucket, 'iris', 'preprocessed', 'features_train.tfrecord.gz')\n",
    "eval_data_path = os.path.join(bucket, 'iris', 'preprocessed', 'features_eval.tfrecord.gz')\n",
    "metadata_path = os.path.join(bucket, 'iris', 'preprocessed', 'metadata.yaml')\n",
    "output_dir = os.path.join(bucket, 'iris', 'trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = 'gs://' + datalab_project_id() + '-sampledata'\n",
    "package_path = os.path.join(bucket, 'iris', 'model', 'trainer-0.1.tar.gz')\n",
    "train_data_path = os.path.join(bucket, 'iris', 'preprocessed', 'features_train.tfrecord.gz')\n",
    "eval_data_path = os.path.join(bucket, 'iris', 'preprocessed', 'features_eval.tfrecord.gz')\n",
    "metadata_path = os.path.join(bucket, 'iris', 'preprocessed', 'metadata.yaml')\n",
    "output_dir = os.path.join(bucket, 'iris', 'trained2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160929_075457\" was submitted successfully.<br/>Run \"%mlalpha jobs --name trainer_task_160929_075457\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-ml-test-automated&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160929_075457\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: $package_path\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - $train_data_path\n",
    "  eval_data_paths:\n",
    "    - $eval_data_path\n",
    "  metadata_path: $metadata_path\n",
    "  output_path: $output_dir\n",
    "  max_steps: 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy trainer package to a GCS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/datalab/tmp/ml/iris/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  4.4 KiB/  4.4 KiB]                                                \n",
      "Operation completed over 1 objects/4.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/datalab/tmp/ml/iris/trainer-0.1.tar.gz $package_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cloud training, there are extra required fields that need to be filled. It needs explicit trainer package so 'package_uris' and 'python_module' are required. 'scale_tier' and 'region' are also required to indicate training scale requirements and location. 'scale_tier: BASIC' means only one master. <br>\n",
    "Note \"--cloud\" indicates it will run in cloud instead of local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160929_035844\" was submitted successfully.<br/>Run \"%mlalpha jobs --name trainer_task_160929_035844\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-ml-test-automated&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160929_035844\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: $package_path\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - $train_data_path\n",
    "  eval_data_paths:\n",
    "    - $eval_data_path\n",
    "  metadata_path: $metadata_path\n",
    "  output_path: $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the job status as described in the output. You can also run \"%mlalpha jobs --filter state!=SUCCEEDED\" to see all active ML jobs in that project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>createTime: '2016-09-29T03:44:33Z'\n",
       "jobId: trainer_task_160929_034432\n",
       "startTime: '2016-09-29T03:46:57Z'\n",
       "state: RUNNING\n",
       "trainingInput:\n",
       "  args: [--train_data_paths, 'gs://cloud-ml-test-automated-sampledata/iris/preprocessed/features_train.tfrecord.gz',\n",
       "    --metadata_path, 'gs://cloud-ml-test-automated-sampledata/iris/preprocessed/metadata.yaml',\n",
       "    --output_path, 'gs://cloud-ml-test-automated-sampledata/iris/trained', --eval_data_paths,\n",
       "    'gs://cloud-ml-test-automated-sampledata/iris/preprocessed/features_eval.tfrecord.gz']\n",
       "  packageUris: ['gs://cloud-ml-test-automated-sampledata/iris/model/trainer-0.1.tar.gz']\n",
       "  pythonModule: trainer.task\n",
       "  region: us-central1\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha jobs --name trainer_task_160929_034432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the trained model once the state is 'SUCCEEDED':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-test-automated-sampledata/iris/trained/\r\n",
      "gs://cloud-ml-test-automated-sampledata/iris/trained/model/\r\n",
      "gs://cloud-ml-test-automated-sampledata/iris/trained/train/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard works too with GCS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 11859. Click <a href=\"/_proxy/41400/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%tensorboard start --logdir $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
