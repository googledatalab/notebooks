{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Commands\n",
    "\n",
    "Google Cloud Datalab provides a set of commands for working with data stored in Google Cloud Storage. They can help you work with data files containing data that is not stored in BigQuery or manage data imported into or exported from BigQuery.\n",
    "\n",
    "This notebook introduces several Cloud Storage commands that Datalab introduces into the notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Commands\n",
    "\n",
    "The commands can list storage buckets and their contained objects, manage those objects, and read from and write to those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: gcs [-h] {copy,create,delete,list,read,view,write} ...\n",
      "\n",
      "Execute various Google Cloud Storage related operations. Use \"%gcs <command>\n",
      "-h\" for help on a specific command.\n",
      "\n",
      "positional arguments:\n",
      "  {copy,create,delete,list,read,view,write}\n",
      "                        commands\n",
      "    copy                Copy one or more Google Cloud Storage objects to a\n",
      "                        different location.\n",
      "    create              Create one or more Google Cloud Storage buckets.\n",
      "    delete              Delete one or more Google Cloud Storage buckets or\n",
      "                        objects.\n",
      "    list                List buckets in a project, or contents of a bucket.\n",
      "    read                Read the contents of a Google Cloud Storage object\n",
      "                        into a Python variable.\n",
      "    view                View the contents of a Google Cloud Storage object.\n",
      "    write               Write the value of a Python variable to a Google Cloud\n",
      "                        Storage object.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%%gcs --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buckets and Objects\n",
    "\n",
    "Items or files held in Cloud Storage are called `objects`. These objects are immutable once written. They are organized into buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a couple of commands to list Datalab sample data. Try `%%gcs list` without arguments to list all buckets within the current project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%gcs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Name</th><th>Type</th><th>Size</th><th>Updated</th></tr><tr><td>applogs</td><td>application/octet-stream</td><td>506050</td><td>2015-11-24 00:06:07.588000+00:00</td></tr><tr><td>carprices/testing.csv</td><td>text/csv</td><td>3635</td><td>2015-10-06 09:02:03.638000+00:00</td></tr><tr><td>carprices/training.csv</td><td>text/csv</td><td>15018</td><td>2015-10-06 09:01:46.040000+00:00</td></tr><tr><td>cars.csv</td><td>text/csv</td><td>248</td><td>2015-10-05 04:58:10.481000+00:00</td></tr><tr><td>cars2.csv</td><td>text/csv</td><td>92</td><td>2015-10-05 05:41:30.935000+00:00</td></tr><tr><td>census/</td><td>application/x-www-form-urlencoded;charset=UTF-8</td><td>0</td><td>2017-03-05 05:51:55.107000+00:00</td></tr><tr><td>census/ACS2014_PUMS_README.pdf</td><td>application/pdf</td><td>289316</td><td>2017-03-05 05:52:31.193000+00:00</td></tr><tr><td>census/ss14psd.csv</td><td>binary/octet-stream</td><td>8189323</td><td>2017-03-05 05:53:54.728000+00:00</td></tr><tr><td>hello.txt</td><td>text/plain</td><td>14</td><td>2015-10-05 04:48:39.433000+00:00</td></tr><tr><td>httplogs/logs20140615.csv</td><td>text/csv</td><td>23799981</td><td>2015-10-06 08:39:42.605000+00:00</td></tr><tr><td>httplogs/logs20140616.csv</td><td>text/csv</td><td>86323745</td><td>2015-10-06 08:39:43.067000+00:00</td></tr><tr><td>httplogs/logs20140617.csv</td><td>text/csv</td><td>51282558</td><td>2015-10-06 08:39:43.622000+00:00</td></tr><tr><td>httplogs/logs20140618.csv</td><td>text/csv</td><td>53380318</td><td>2015-10-06 08:39:44.191000+00:00</td></tr><tr><td>httplogs/logs20140619.csv</td><td>text/csv</td><td>87691363</td><td>2015-10-06 08:39:44.794000+00:00</td></tr><tr><td>httplogs/logs20140620.csv</td><td>text/csv</td><td>47229334</td><td>2015-10-06 08:39:45.236000+00:00</td></tr><tr><td>httplogs/logs_sample.csv</td><td>text/csv</td><td>3949</td><td>2015-10-06 08:39:45.729000+00:00</td></tr><tr><td>stackdriver-monitoring/timeseries/per-zone-weekly-20161010.csv</td><td>text/csv</td><td>8725</td><td>2016-10-13 16:09:21.470000+00:00</td></tr><tr><td>stackdriver-monitoring/timeseries/topic-message-sizes-20161208.csv</td><td>text/csv</td><td>114702</td><td>2016-12-08 21:36:21.500000+00:00</td></tr><tr><td>udfsample/</td><td>application/x-www-form-urlencoded;charset=UTF-8</td><td>0</td><td>2015-11-23 23:57:38.494000+00:00</td></tr><tr><td>udfsample/2015_station_data.csv</td><td>text/csv</td><td>4230</td><td>2015-11-24 00:20:14.575000+00:00</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%gcs list --objects gs://cloud-datalab-samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use wildchars to list all objects matching a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Name</th><th>Type</th><th>Size</th><th>Updated</th></tr><tr><td>udfsample/</td><td>application/x-www-form-urlencoded;charset=UTF-8</td><td>0</td><td>2015-11-23 23:57:38.494000+00:00</td></tr><tr><td>udfsample/2015_station_data.csv</td><td>text/csv</td><td>4230</td><td>2015-11-24 00:20:14.575000+00:00</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%gcs list --objects gs://cloud-datalab-samples/udf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: gs://mysampleproject-datalab-samples\n",
      "Object: gs://mysampleproject-datalab-samples/Hello.txt\n"
     ]
    }
   ],
   "source": [
    "# Some code to determine a unique bucket name for the purposes of the sample\n",
    "from google.datalab import Context\n",
    "\n",
    "project = Context.default().project_id\n",
    "sample_bucket_name = project + '-datalab-samples'\n",
    "sample_bucket_path = 'gs://' + sample_bucket_name\n",
    "sample_bucket_object = sample_bucket_path + '/Hello.txt'\n",
    "\n",
    "print 'Bucket: ' + sample_bucket_path\n",
    "print 'Object: ' + sample_bucket_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In the examples below, the variables are referenced in the command using `$` syntax since the names are determined based on the current project. In your scenarios, you may be able to use literal values if they are constant instead of creating and using variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%gcs create --bucket $sample_bucket_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%gcs list --objects $sample_bucket_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%gcs copy --source gs://cloud-datalab-samples/hello.txt --destination $sample_bucket_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Name</th><th>Type</th><th>Size</th><th>Updated</th></tr><tr><td>Hello.txt</td><td>text/plain</td><td>14</td><td>2017-03-07 10:03:16.808000+00:00</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%gcs list --objects $sample_bucket_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World!\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%gcs view --object $sample_bucket_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%gcs read --object $sample_bucket_object --variable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Hello World!\\n====\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%gcs write --variable text --object $sample_bucket_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Name</th><th>Type</th><th>Size</th><th>Updated</th></tr><tr><td>Hello.txt</td><td>text/plain</td><td>18</td><td>2017-03-07 10:03:49.868000+00:00</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%gcs list --objects $sample_bucket_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%gcs delete --object $sample_bucket_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%gcs delete --bucket $sample_bucket_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking Ahead\n",
    "\n",
    "The above Cloud Storage commands build on the Storage APIs included in Datalab. Another notebook demonstrates these APIs.\n",
    "\n",
    "Also, BigQuery functionality supports exporting data to and importing data from Cloud Storage, as shown in the BigQuery tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
