{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "Once you have gathered your data and decided how to preprocess them (a featureset class is already defined), we can preprocess the data. One way to preprocess the data is to use DataFlow. If your data is large, DataFlow can run in cloud in a distributed fashion. If not large, you can also run the DataFlow locally. <br><br>\n",
    "\n",
    "CloudML provides a preprocess DataFlow transformation so it can be easily plugged into the pipeline.\n",
    "\n",
    "What Datalab provides is generated code template with \"%mlalpha preprocess\" command, so you don't have to start from scratch to author your DataFlow pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing requires a featureset class. We've done that in previous \"1.Feature\" notebook but we need to define it again here in this notebook scope.\n",
    "Note that we choose to preprocess all numeric feature columns with [-1, 1] scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "class IrisFeatures(object):\n",
    "  \"\"\"This class is generated from command line:\n",
    "        %ml features\n",
    "        path: /content/datalab/tmp/ml/iris/data_train.csv\n",
    "        headers: key,species,sepal_length,sepal_width,petal_length,petal_width\n",
    "        target: species\n",
    "        id: key\n",
    "        Please modify it as appropriate!!!\n",
    "  \"\"\"\n",
    "  csv_columns = ('key','species','sepal_length','sepal_width','petal_length','petal_width')\n",
    "  species = features.target('species').discrete()\n",
    "  key = features.key('key')\n",
    "  measurements = [\n",
    "      features.numeric('petal_width').max_abs_scale(1),\n",
    "      features.numeric('sepal_length').max_abs_scale(1),\n",
    "      features.numeric('petal_length').max_abs_scale(1),\n",
    "      features.numeric('sepal_width').max_abs_scale(1),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run %mlalpha preprocess, and it generates the input cell for you to fill out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mlalpha preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the cell input: so it looks like\n",
    "```\n",
    "%%mlalpha preprocess\n",
    "train_data_path: /content/datalab/tmp/ml/iris/data_train.csv\n",
    "eval_data_path: /content/datalab/tmp/ml/iris/data_eval.csv\n",
    "data_format: CSV\n",
    "output_dir: /content/datalab/tmp/ml/iris/preprocessed\n",
    "feature_set_class_name: IrisFeatures\n",
    "```\n",
    "\n",
    "And then run it. Next cell is what you get. You can run the pipeline directly (it is a local pipeline), or extend it with more DataFlow transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7fce846aa850>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%mlalpha preprocess\n",
    "train_data_path: /content/datalab/tmp/ml/iris/data_train.csv\n",
    "eval_data_path: /content/datalab/tmp/ml/iris/data_eval.csv\n",
    "data_format: CSV\n",
    "output_dir: /content/datalab/tmp/ml/iris/preprocessed\n",
    "feature_set_class_name: IrisFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = IrisFeatures()\n",
    "OUTPUT_DIR = '/content/datalab/tmp/ml/iris/preprocessed'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    '/content/datalab/tmp/ml/iris/data_train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    '/content/datalab/tmp/ml/iris/data_eval.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline  | beam.Read('ReadEvalData', eval_data)\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) | 'Preprocess'\n",
    "    >> ml.Preprocess(feature_set, input_format='csv',\n",
    "                  format_metadata={'headers': feature_set.csv_columns}))\n",
    "\n",
    "(metadata        | 'SaveMetadata'\n",
    "    >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "\n",
    "(train_features  | 'SaveTrain'\n",
    "    >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train'), shard_name_template=''))\n",
    "\n",
    "(eval_features   | 'SaveEval'\n",
    "    >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval'), shard_name_template=''))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline without modification, and check the output. The output is compressed TF Record format that can be consumed by TensorFlow directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_eval.tfrecord.gz  features_train.tfrecord.gz  metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/datalab/tmp/ml/iris/preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Preprocessing\n",
    "You can also generate Cloud DataFlow pipeline. Just add \"--cloud\" to \"%ml preprocess\". <br>\n",
    "Note that if you need to get it running in cloud, you need: <br>\n",
    "1. Sign In using the up right sign-in button, if you have not done so. <br>\n",
    "2. Set a default project by running '%projects set Your-Project-Id'.\n",
    "3. Your data need to be in Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = 'gs://' + datalab_project_id() + '-sampledata'\n",
    "train_data_path = os.path.join(bucket, 'iris', 'data_train.csv')\n",
    "eval_data_path = os.path.join(bucket, 'iris', 'data_eval.csv')\n",
    "output_dir = os.path.join(bucket, 'iris', 'preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GCS bucket and copy training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%storage create --bucket $bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-datalab/sampledata/ml/iris/data_train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 1 objects/3.8 KiB.                                      \n",
      "Copying gs://cloud-datalab/sampledata/ml/iris/data_train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 1 objects/3.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-datalab/sampledata/ml/iris/data_train.csv $train_data_path\n",
    "!gsutil cp gs://cloud-datalab/sampledata/ml/iris/data_train.csv $eval_data_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the input before it generates the pipeline code.\n",
    "```\n",
    "%%mlalpha preprocess --cloud\n",
    "train_data_path: $train_data_path\n",
    "eval_data_path: $eval_data_path\n",
    "data_format: CSV\n",
    "output_dir: $output_dir\n",
    "feature_set_class_name: IrisFeatures\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It generates a Cloud DataFlow pipeline. Run the code and it will start DataFlow in Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Direct usage of TextFileSink is deprecated. Please use 'textio.WriteToText()' instead of directly instantiating a TextFileSink object.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/usr/bin/python', '-m', 'pip', 'install', '--download', '/tmp/tmpto6OzZ', 'google-cloud-dataflow==0.4.2.dev0', '--no-binary', ':all:', '--no-deps']' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-50b0a896f24f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# run pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow_runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    170\u001b[0m         pipeline.options, job_version)\n\u001b[1;32m    171\u001b[0m     self.result = DataflowPipelineResult(\n\u001b[0;32m--> 172\u001b[0;31m         self.dataflow_client.create_job(self.job))\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;31m# Stage job resources and add an environment proto with their paths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     resources = dependency.stage_job_resources(\n\u001b[0;32m--> 375\u001b[0;31m         job.options, file_copy=self._gcs_file_copy)\n\u001b[0m\u001b[1;32m    376\u001b[0m     job.proto.environment = Environment(\n\u001b[1;32m    377\u001b[0m         \u001b[0mpackages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/dependency.pyc\u001b[0m in \u001b[0;36mstage_job_resources\u001b[0;34m(options, file_copy, build_setup_args, temp_dir, populate_requirements_cache)\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0msdk_remote_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m       \u001b[0m_stage_dataflow_sdk_tarball\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdk_remote_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstaged_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m       \u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATAFLOW_SDK_TARBALL_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/dependency.pyc\u001b[0m in \u001b[0;36m_stage_dataflow_sdk_tarball\u001b[0;34m(sdk_remote_location, staged_path, temp_dir)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msdk_remote_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pypi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Staging the SDK tarball from PyPI to %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstaged_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0m_dependency_file_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_download_pypi_sdk_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstaged_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/dependency.pyc\u001b[0m in \u001b[0;36m_download_pypi_sdk_package\u001b[0;34m(temp_dir)\u001b[0m\n\u001b[1;32m    494\u001b[0m       '--no-binary', ':all:', '--no-deps']\n\u001b[1;32m    495\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Executing command: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m   \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m   zip_expected = os.path.join(\n\u001b[1;32m    498\u001b[0m       temp_dir, '%s-%s.zip' % (GOOGLE_PACKAGE_NAME, version))\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/processes.pyc\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mforce_shell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/usr/bin/python', '-m', 'pip', 'install', '--download', '/tmp/tmpto6OzZ', 'google-cloud-dataflow==0.4.2.dev0', '--no-binary', ':all:', '--no-deps']' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%mlalpha preprocess --cloud\n",
    "train_data_path: $train_data_path\n",
    "eval_data_path: $eval_data_path\n",
    "data_format: CSV\n",
    "output_dir: $output_dir\n",
    "feature_set_class_name: IrisFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = IrisFeatures()\n",
    "OUTPUT_DIR = 'gs://cloud-ml-test-automated-sampledata/iris/preprocessed'\n",
    "import datetime\n",
    "options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-irisfeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': 'cloud-ml-test-automated',\n",
    "    'extra_packages': ['gs://cloud-ml/sdk/cloudml-0.1.6-alpha.tar.gz'],\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "pipeline = beam.Pipeline('DataflowPipelineRunner', options=opts)\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    'gs://cloud-ml-test-automated-sampledata/iris/data_train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    'gs://cloud-ml-test-automated-sampledata/iris/data_eval.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline  | beam.Read('ReadEvalData', eval_data)\n",
    "\n",
    "(metadata, train_features, eval_features) = ((train, eval) | 'Preprocess'\n",
    "    >> ml.Preprocess(feature_set, input_format='csv',\n",
    "                  format_metadata={'headers': feature_set.csv_columns}))\n",
    "\n",
    "(metadata        | 'SaveMetadata'\n",
    "    >> io.SaveMetadata(os.path.join(OUTPUT_DIR, 'metadata.yaml')))\n",
    "\n",
    "(train_features  | 'SaveTrain'\n",
    "    >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_train'), shard_name_template=''))\n",
    "\n",
    "(eval_features   | 'SaveEval'\n",
    "    >> io.SaveFeatures(os.path.join(OUTPUT_DIR, 'features_eval'), shard_name_template=''))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run the above generated code, you can go to Developer Console to see the DataFlow job: https://pantheon.corp.google.com/dataflow (and select the right project). Also, run the following to make sure the files were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-test-automated-sampledata/iris/preprocessed/features_eval.tfrecord.gz\r\n",
      "gs://cloud-ml-test-automated-sampledata/iris/preprocessed/features_train.tfrecord.gz\r\n",
      "gs://cloud-ml-test-automated-sampledata/iris/preprocessed/metadata.yaml\r\n",
      "gs://cloud-ml-test-automated-sampledata/iris/preprocessed/tmp/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
