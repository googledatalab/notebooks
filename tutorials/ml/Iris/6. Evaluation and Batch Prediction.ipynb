{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is still under construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Batch prediction and evaluation are very similar. They are based on DataFlow pipeline and CloudML provides Evaluate and Prediction DataFlow transform. Datalab can generate DataFlow pipeline code template for you, just like Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"%mlalpha evaluate\" to generate input cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mlalpha evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fill in the required fields, you will have:\n",
    "```\n",
    "%%ml evaluate\n",
    "preprocessed_eval_data_path: /content/datalab/tmp/ml/iris/preprocessed/features_eval.tfrecord.Z\n",
    "metadata_path: /content/datalab/tmp/ml/iris/preprocessed/metadata.yaml\n",
    "model_dir: /content/datalab/tmp/ml/iris/model/model\n",
    "output_dir: /content/datalab/tmp/ml/iris/evaluate\n",
    "output_prediction_name: predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the generated code. Optionally uncomment the code for creating confusion matrix plot. Note that the confusionmatrix code is only generated in classification case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%mlalpha evaluate\n",
    "preprocessed_eval_data_path: /content/datalab/tmp/ml/iris/preprocessed/features_eval.tfrecord.gz\n",
    "metadata_path: /content/datalab/tmp/ml/iris/preprocessed/metadata.yaml\n",
    "model_dir: /content/datalab/tmp/ml/iris/model/model\n",
    "output_dir: /content/datalab/tmp/ml/iris/evaluate\n",
    "output_prediction_name: predictions\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import fileio\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.analysis as analysis\n",
    "import google.cloud.ml.io as io\n",
    "import json\n",
    "import os\n",
    "\n",
    "# defines\n",
    "def extract_values((example, prediction)):\n",
    "  import tensorflow as tf\n",
    "  tf_example = tf.train.Example()\n",
    "  tf_example.ParseFromString(example.values()[0])\n",
    "  feature_map = tf_example.features.feature\n",
    "  values = {'target': feature_map['species'].int64_list.value[0]}\n",
    "  values.update(prediction)\n",
    "  return values\n",
    "\n",
    "OUTPUT_DIR = '/content/datalab/tmp/ml/iris/evaluate'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# evaluation\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix='/content/datalab/tmp/ml/iris/preprocessed/features_eval.tfrecord.gz',\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "eval_features = pipeline | io.LoadFeatures('LoadEvalFeatures', eval_parameters)\n",
    "trained_model = pipeline | io.LoadModel('LoadModel', '/content/datalab/tmp/ml/iris/model/model')\n",
    "evaluations = (eval_features | ml.Evaluate(trained_model, label='Evaluate')\n",
    "    | beam.Map('ExtractEvaluationResults', extract_values))\n",
    "eval_data_sink = beam.io.TextFileSink(os.path.join(OUTPUT_DIR, 'eval'), shard_name_template='')\n",
    "evaluations | beam.Write('WriteEval', eval_data_sink)\n",
    "\n",
    "# analysis\n",
    "def make_data_for_analysis(values):\n",
    "  return {\n",
    "      'target': values['target'],\n",
    "      'predicted': values['predictions'],\n",
    "      'score': 0.0,\n",
    "  }\n",
    "\n",
    "metadata = pipeline | io.LoadMetadata('/content/datalab/tmp/ml/iris/preprocessed/metadata.yaml')\n",
    "analysis_source = evaluations | beam.Map('CreateAnalysisSource', make_data_for_analysis)\n",
    "confusion_matrix, precision_recall, logloss = (analysis_source |\n",
    "    analysis.AnalyzeModel('Analyze Model', metadata))\n",
    "confusion_matrix_file = os.path.join(OUTPUT_DIR, 'analyze_cm.json')\n",
    "confusion_matrix_sink = beam.io.TextFileSink(confusion_matrix_file, shard_name_template='')\n",
    "confusion_matrix | beam.io.Write('WriteConfusionMatrix', confusion_matrix_sink)\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n",
    "\n",
    "# View Confusion Matrix with the following code:\n",
    "#\n",
    "# import datalab.mlalpha\n",
    "# import yaml\n",
    "# with ml.util._file.open_local_or_gcs(confusion_matrix_file, 'r') as f:\n",
    "#   data = [yaml.load(line) for line in f.read().rstrip().split('\\n')]\n",
    "# datalab.mlalpha.ConfusionMatrix([d['predicted'] for d in data],\n",
    "#                            [d['target'] for d in data],\n",
    "#                            [d['count'] for d in data]).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check the eval output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 2L, u'key': ['107'], u'scores': [0.0002932963252533227, 0.4270738661289215, 0.5726327896118164]}\r\n",
      "{'target': 1L, u'key': ['100'], u'scores': [0.002486891346052289, 0.9954042434692383, 0.002108802553266287]}\r\n",
      "{'target': 1L, u'key': ['99'], u'scores': [0.04234505072236061, 0.9572798609733582, 0.0003750604810193181]}\r\n",
      "{'target': 0L, u'key': ['13'], u'scores': [0.997825026512146, 0.002174961846321821, 3.2759878365595796e-08]}\r\n",
      "{'target': 1L, u'key': ['70'], u'scores': [0.0018489608773961663, 0.9972803592681885, 0.000870543357450515]}\r\n",
      "{'target': 0L, u'key': ['11'], u'scores': [0.9992808699607849, 0.0007191312615759671, 1.3344539695481217e-08]}\r\n",
      "{'target': 0L, u'key': ['37'], u'scores': [0.998814582824707, 0.0011853966861963272, 1.951879369244125e-08]}\r\n",
      "{'target': 1L, u'key': ['69'], u'scores': [4.0491268009645864e-05, 0.261225163936615, 0.7387343645095825]}\r\n",
      "{'target': 0L, u'key': ['40'], u'scores': [0.9988722205162048, 0.0011277850717306137, 2.4214845240067007e-08]}\r\n",
      "{'target': 2L, u'key': ['101'], u'scores': [7.731774331887209e-08, 0.00016101982328109443, 0.9998388290405273]}\r\n"
     ]
    }
   ],
   "source": [
    "!head /content/datalab/tmp/ml/iris/evaluate/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a pipeline that runs in cloud, simply run \"%ml evaluate --cloud\". Also all paths need to be GCS paths. Let's define the variables first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = 'gs://' + datalab_project_id() + '-sampledata'\n",
    "eval_data_path = os.path.join(bucket, 'iris', 'preprocessed', 'features_eval')\n",
    "metadata_path = os.path.join(bucket, 'iris', 'preprocessed', 'metadata.yaml')\n",
    "model_path = os.path.join(bucket, 'iris', 'trained', 'model')\n",
    "output_dir = os.path.join(bucket, 'iris', 'evaluate')\n",
    "eval_file = os.path.join(output_dir, 'eval*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run it to generate the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml evaluate --cloud\n",
    "preprocessed_eval_data_path: $eval_data_path\n",
    "metadata_path: $metadata_path\n",
    "model_dir: $model_path\n",
    "output_dir: $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated code is like the following:\n",
    "\n",
    "```\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%ml evaluate --cloud\n",
    "preprocessed_eval_data_path: $eval_data_path\n",
    "metadata_path: $metadata_path\n",
    "model_dir: $model_path\n",
    "output_dir: $output_dir\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import fileio\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.analysis as analysis\n",
    "import google.cloud.ml.dataflow.io.tfrecordio as tfrecordio\n",
    "import google.cloud.ml.io as io\n",
    "import json\n",
    "import os\n",
    "\n",
    "# defines\n",
    "def extract_values((example, prediction)):\n",
    "  import tensorflow as tf\n",
    "  tf_example = tf.train.Example()\n",
    "  tf_example.ParseFromString(example.values()[0])\n",
    "  feature_map = tf_example.features.feature\n",
    "  values = {'target': feature_map['species'].int64_list.value[0]}\n",
    "  values.update(prediction)\n",
    "  return values\n",
    "\n",
    "OUTPUT_DIR = 'gs://cloud-ml-test-automated-sampledata/iris/evaluate'\n",
    "import datetime\n",
    "options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'evaluate' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': 'cloud-ml-test-automated',\n",
    "    'extra_packages': ['gs://cloud-ml/sdk/cloudml-0.1.2.latest.tar.gz'],\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "pipeline = beam.Pipeline('DataflowPipelineRunner', options=opts)\n",
    "\n",
    "\n",
    "# evaluation\n",
    "eval_parameters = tfrecordio.TFRecordParameters(\n",
    "    file_path_prefix='gs://cloud-ml-test-automated-sampledata/iris/preprocessed/features_eval',\n",
    "    file_name_suffix='',\n",
    "    shard_file=False,\n",
    "    compress_file=True)\n",
    "eval_features = pipeline | io.LoadFeatures('LoadEvalFeatures', eval_parameters)\n",
    "trained_model = pipeline | io.LoadModel('LoadModel', 'gs://cloud-ml-test-automated-sampledata/iris/trained/model')\n",
    "evaluations = (eval_features | ml.Evaluate(trained_model, label='Evaluate')\n",
    "    | beam.Map('ExtractEvaluationResults', extract_values))\n",
    "eval_data_sink = beam.io.TextFileSink(os.path.join(OUTPUT_DIR, 'eval'))\n",
    "evaluations | beam.Write('WriteEval', eval_data_sink)\n",
    "\n",
    "# analysis\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run the above generated code, you can go to Developer Console to see the DataFlow job: https://pantheon.corp.google.com/dataflow (and select the right project). Also you can check the results as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'score': [0.9643456935882568, 0.03509025275707245, 0.000563996727578342], 'target': 0L, u'key': ['4'], u'predictions': 0}\n",
      "{u'score': [0.985572099685669, 0.013915198855102062, 0.0005126940086483955], 'target': 0L, u'key': ['20'], u'predictions': 0}\n",
      "{u'score': [0.9754565358161926, 0.024147897958755493, 0.00039562786696478724], 'target': 0L, u'key': ['43'], u'predictions': 0}\n",
      "{u'score': [0.019101984798908234, 0.7742735147476196, 0.20662443339824677], 'target': 1L, u'key': ['88'], u'predictions': 1}\n",
      "{u'score': [0.05851663649082184, 0.6017542481422424, 0.33972907066345215], 'target': 1L, u'key': ['76'], u'predictions': 1}\n",
      "{u'score': [0.038775887340307236, 0.9083987474441528, 0.05282538756728172], 'target': 1L, u'key': ['63'], u'predictions': 1}\n",
      "{u'score': [0.9870647192001343, 0.012523564510047436, 0.00041171154589392245], 'target': 0L, u'key': ['47'], u'predictions': 0}\n",
      "{u'score': [0.0019517333712428808, 0.08908909559249878, 0.9089592099189758], 'target': 2L, u'key': ['146'], u'predictions': 2}\n",
      "{u'score': [0.029388712719082832, 0.3492112159729004, 0.6214000582695007], 'target': 1L, u'key': ['53'], u'predictions': 2}\n",
      "{u'score': [0.013716676272451878, 0.33357346057891846, 0.6527097821235657], 'target': 1L, u'key': ['71'], u'predictions': 2}\n",
      "close failed in file object destructor:\n",
      "sys.excepthook is missing\n",
      "lost sys.stderr\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat $eval_file | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
